\documentclass[a4paper,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{wallpaper}
\usepackage[absolute]{textpos}
\usepackage[top=2cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{appendix}
\usepackage[nottoc]{tocbibind}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    citecolor=black
}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}
% Compact table of contents
\makeatletter
\renewcommand{\@tocrmarg}{1.3em plus1fil}
\renewcommand{\@pnumwidth}{0.8em}
\renewcommand{\l@section}{\@dottedtocline{1}{0em}{1.2em}}
\renewcommand{\l@subsection}{\@dottedtocline{2}{1.2em}{2.0em}}
\makeatother
\usepackage{sectsty}
\sectionfont{\fontsize{14}{15}\selectfont}
\subsectionfont{\fontsize{12}{15}\selectfont}
\subsubsectionfont{\fontsize{12}{15}\selectfont}
\usepackage{csquotes}
\usepackage{listings,xcolor}
\definecolor{vscBg}{HTML}{1E1E1E}
\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage{float}
\usepackage[final]{microtype}


% VS Code Dark+ palette (extended)
\definecolor{vscBg}   {HTML}{1E1E1E}  % background
\definecolor{vscTxt}  {HTML}{D4D4D4}  % plain text
\definecolor{vscKw}   {HTML}{569CD6}  % keywords
\definecolor{vscFn}   {HTML}{DCDCAA}  % function names / built-ins
\definecolor{vscStr}  {HTML}{CE9178}  % strings
\definecolor{vscCmt}  {HTML}{6A9955}  % comments
\definecolor{vscNum}  {HTML}{B5CEA8}  % numbers

\lstdefinestyle{vscode}{
  language=Python,
  backgroundcolor=\color{vscBg},
  basicstyle=\ttfamily\small\color{vscTxt},
  keywordstyle=\color{vscKw}\bfseries,
  stringstyle=\color{vscStr},
  commentstyle=\color{vscCmt}\itshape,
  numberstyle=\color{vscNum},
  emphstyle=\color{vscFn},              % for emph (see below)
  numbers=left,
  stepnumber=1,
  numbersep=6pt,
  showstringspaces=false,
  frame=single,
  rulecolor=\color{vscBg},
  captionpos=b,
  breaklines=true,     
  breakindent=1.5em 
}

\renewcommand{\thetable}{\arabic{section}.\arabic{table}}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}
\newsavebox{\mybox}
\newlength{\mydepth}
\newlength{\myheight}
\newenvironment{sidebar}{\begin{lrbox}{\mybox}\begin{minipage}{\textwidth}}{\end{minipage}\end{lrbox}\settodepth{\mydepth}{\usebox{\mybox}}\settoheight{\myheight}{\usebox{\mybox}}\addtolength{\myheight}{\mydepth}\noindent\makebox[0pt]{\hspace{-20pt}\rule[-\mydepth]{1pt}{\myheight}}\usebox{\mybox}}
\newcommand\BackgroundPic{
    \put(-2,-3){
    \includegraphics[keepaspectratio,scale=0.3]{img/lnu_etch.png}
    }
}
\newcommand\BackgroundPicLogo{
    \put(30,740){
    \includegraphics[keepaspectratio,scale=0.10]{img/logo.png}
    }
}
\title{
\vspace{-8cm}
\begin{sidebar}
    \vspace{10cm}
    \normalfont \normalsize
    \Huge Bachelor Degree Project \\
    \vspace{-1.3cm}
\end{sidebar}
\vspace{3cm}
\begin{flushleft}
    \huge Voice Enabled Command Line Interface with Speech Recognition and LLM Support\\ 
    % \it \LARGE - Optional subtitle 
\end{flushleft}
\null
\vfill
\begin{textblock}{6}(10,12)
\begin{flushright}
\begin{minipage}{\textwidth}
\begin{flushleft} \large
\emph{Author(s):} Earmyas Measho Gebre and Shoeb Chowdhury\\
\emph{Supervisor(s):} Rafael Martins\\ 
\emph{Semester:} VT 2025\\
\emph{Course:} 2DV50E \\
\emph{Subject:} Computer Science \\
\end{flushleft}
\end{minipage}
\end{flushright}
\end{textblock}
}
\date{}



% Better hyphenation and line breaking
\sloppy
\emergencystretch=1em
\hyphenpenalty=5000
\exhyphenpenalty=5000

% Proper handling of units
\usepackage{siunitx}
\DeclareSIUnit{\s}{s} % For seconds

% Better handling of long URLs and technical terms
\usepackage[hyphens]{url}
\urlstyle{same}

\begin{document}
\pagenumbering{gobble}
\newgeometry{left=5cm}
\AddToShipoutPicture*{\BackgroundPic}
\AddToShipoutPicture*{\BackgroundPicLogo}
\maketitle
\restoregeometry
\selectlanguage{english}

\begin{abstract}
\noindent Command-line interfaces (CLIs), while powerful, present substantial accessibility and usability challenges due to their reliance on memorized syntax and manual typing, as noted in usability evaluation frameworks~\cite{ref16}. This thesis introduces \textit{VoiceCLI}, a voice-driven command-line assistant for Windows shell environments designed to mitigate these barriers. VoiceCLI's architecture integrates local Automatic Speech Recognition (ASR) using Whisper for privacy, a cloud-based Large Language Model (LLM)—Mistral-7B—for command translation, and an explicit confirmation interface for safety. The system's efficacy was evaluated through a controlled user study involving \textbf{25 participants} who performed \textbf{500 total trials} (20 tasks each) using a novel benchmark dataset, \textit{VoiceShell-100}, which includes commands of stratified difficulty. The evaluation yielded an overall task success rate of \textbf{73.20\%}, with \textbf{58.60\%} of tasks successfully completed on the first attempt. This system-level performance is supported by strong component-level metrics: Whisper achieved \textbf{94.23\%} word-level transcription accuracy, and Mistral-7B demonstrated an \textbf{83.00\%} top-3 command suggestion accuracy. The average end-to-end latency was approximately \textbf{8.30 seconds}, a duration that participants found acceptable for this interaction paradigm. The primary contributions of this work are: (1) a reproducible, open-source prototype of VoiceCLI; (2) empirical benchmark data from a structured usability study; (3) the \textit{VoiceShell-100} task set for future research; and (4) a set of design recommendations for building privacy-conscious, confirmation-driven voice automation systems. Future work directions include: (1) fine-tuning Whisper on CLI-specific terminology to reduce the share of ASR-caused failures (currently 35.07\%) to under 25\%, (2) implementing ReAct-style clarification dialogues to resolve 25-35\% of ambiguous commands, (3) exploring 4-bit quantization for on-device LLM inference to reduce latency from 8.30s to under 5.00s, and (4) adding multilingual support for non-English CLI environments.

\vspace{1em}

\noindent\textbf{Keywords:} voice interface, speech recognition, large language models, command line, accessibility, usability, Whisper, Mistral-7B
\end{abstract}

\newpage
\pagenumbering{gobble}
\vspace*{-1.8cm}
\setlength{\parskip}{0pt}
\tableofcontents
\newpage
\pagenumbering{arabic}

% ======================================================================
%  INTRODUCTION 
% ======================================================================

\newpage
\section{Introduction}

\noindent
Command-line interfaces (CLIs) have been foundational to software development and system administration for over five decades~\cite{ref1}. A global developer survey of over 65,000 developers revealed that 76\% are using or planning to use AI tools in their development process, with 62\% currently using AI tools~\cite{ref2}. While powerful, CLIs introduce substantial usability and accessibility costs~\cite{ref3}, as outlined in modern Human-Computer Interaction (HCI) principles for interface design~\cite{ref10}. Novice users must memorize terse command syntax, and even experienced professionals frequently consult manual pages~\cite{ref1}. For users with disabilities, these barriers are even more pronounced: visual impairments require navigating verbose outputs with screen readers~\cite{ref3}, while motor impairments cause fatigue from prolonged typing~\cite{ref4}.

Voice user interfaces (VUIs) offer a compelling solution to these challenges~\cite{ref5}. For users with disabilities, voice input can eliminate reliance on physical keyboards and complement assistive technologies like screen readers, offering a more direct and less strenuous method of interaction~\cite{ref4}. Recent studies have shown that people with cognitive or speech impairments face unique usability challenges with voice assistants~\cite{ref28}. Beyond accessibility, VUIs offer broad convenience. Speaking natural-language goals (e.g., "list files in the current directory") is often more intuitive than recalling precise syntax. This convenience has driven widespread adoption~\cite{ref7}.

Nevertheless, commercial voice assistants such as Siri, Alexa, and Google Assistant are designed primarily for constrained, consumer-facing tasks~\cite{ref8}. They lack the ability to generate arbitrary shell commands, offer limited support for clarification, and typically stream audio to external servers—posing substantial privacy concerns in security-sensitive environments~\cite{ref7,ref9}.

To address these limitations, we introduce \textit{VoiceCLI}, a hybrid, privacy-conscious voice-driven CLI assistant. VoiceCLI uses Whisper~\cite{ref10}, an open-source automatic speech recognition (ASR) model developed by OpenAI, to transcribe speech locally, ensuring that raw audio never leaves the user's device. For command generation, it leverages Mistral-7B~\cite{ref11}, a state-of-the-art large language model (LLM) hosted on a secure cloud endpoint via the Mistral API with HTTPS encryption and certificate pinning. A critical feature is the explicit confirmation layer, which provides a safety guarantee by requiring user approval before executing any generated command, guided by human-AI interaction principles~\cite{ref12}.

Our evaluation of VoiceCLI involved a controlled study with 25 participants performing 500 voice-based CLI tasks. The study examines effectiveness, recognition accuracy, latency, and perceived usability across a range of task complexities and user backgrounds. Moreover, our findings suggest that confirmation-centric, hybrid ASR–LLM pipelines can make CLI environments more accessible, efficient, and safe for diverse user groups.


\subsection{Background}

Voice user interfaces (VUIs) have become increasingly mainstream due to notable advances in automatic speech recognition (ASR) and the proliferation of microphone-equipped devices~\cite{ref7}. Mainstream voice assistants such as Siri, Alexa, and Google Assistant have demonstrated the convenience and efficiency of spoken interaction for consumer tasks. However, they are not designed for open-ended command execution or domain-specific tasks such as interacting with a command-line interface (CLI). Additionally, their cloud-based processing raises privacy concerns~\cite{ref8,ref9}. These limitations typically preclude their use in professional environments, but VoiceCLI addresses these concerns through its hybrid architecture.



CLIs remain a powerful yet underexplored domain for voice interaction. Despite their flexibility and expressiveness, CLIs pose usability and accessibility challenges, particularly for novice users and those with motor or visual impairments. Memorizing complex syntax, interpreting cryptic error messages, and navigating dense textual output can be difficult or even prohibitive without expert knowledge or physical dexterity. These barriers make CLIs a compelling target for voice augmentation~\cite{ref2,ref4}.

To address these challenges, the VoiceCLI system combines the strengths of VUIs with the expressive power of CLIs, offering a hybrid interface that balances usability, safety, and privacy. By integrating local ASR, cloud-hosted language models, and a confirmation-centric execution model, VoiceCLI aims to make CLI environments more accessible to a wider range of users while preserving the control and precision valued by experts.

\subsection{Technical Background}
To provide context for the design and implementation of VoiceCLI, this section introduces key technical terms and concepts relevant to the thesis. These include automatic speech recognition (ASR), large language models (LLMs), and datasets used for natural language-to-command translation.

\textbf{Automatic Speech Recognition (ASR).} ASR refers to the technology that converts spoken language into written text. Modern systems have achieved high accuracy due to advancements in deep learning and large-scale training data~\cite{ref5}. A notable example is \textit{Whisper}, a robust ASR model developed by OpenAI~\cite{ref10}. Whisper supports multilingual transcription and operates offline, ensuring that raw audio never leaves the user's device—a critical requirement for privacy-sensitive applications such as VoiceCLI.

\textbf{Large Language Models (LLMs).} LLMs are machine learning models trained on massive text corpora to generate human-like language and perform tasks such as translation, summarization, and command generation. For this project, we selected \textbf{Mistral-7B}, a state-of-the-art open-weight model capable of interpreting natural language instructions and producing shell commands~\cite{ref11}. Mistral-7B was chosen for its availability, free API access, and performance characteristics suitable for real-time command generation.



\textbf{Natural Language-to-Command Datasets.} Two primary datasets underlie this domain: \textbf{NL2Bash} and \textbf{NLC2CMD}. NL2Bash maps English sentences to Bash commands for Linux environments~\cite{ref26}, while NLC2CMD targets Windows command translation~\cite{ref27}. VoiceShell-100 extends these datasets with Windows-specific tasks, creating a balanced benchmark for evaluation. All three datasets were used to inspire VoiceCLI's task taxonomy and benchmark command generation performance. This work builds on foundational research in natural language processing for information retrieval~\cite{ref14}, adapting these principles for command-line interface interaction.

\textbf{Voice User Interfaces (VUIs).} VUIs allow users to interact with software through speech instead of traditional input devices. Although popular in consumer contexts (e.g., smartphones, smart speakers), their application to developer tools like the command line remains limited~\cite{ref7}. Unlike Siri or Alexa, which rely on cloud-based processing, VoiceCLI emphasizes privacy by performing ASR locally and only transmitting text to a secure LLM endpoint. This multimodal approach, combining speech input with visual feedback, aligns with established HCI principles~\cite{ref6}.

\textbf{Confirmation-Centric Design.} VoiceCLI follows a confirmation-centric approach, ensuring no command is executed without explicit user approval. This reduces the risk of errors due to misrecognition or flawed suggestions. For potentially destructive commands (e.g., `rm -rf`, `format`), full confirmation phrases such as "yes" are required. This design aligns with best practices in human-computer interaction~\cite{ref12} and considers the context of user interactions~\cite{ref13}. It also reinforces trust in AI-assisted systems.


\subsection{Related Work}

\noindent Our research builds on decades of work in voice interfaces and command-line systems. The following discussion examines existing solutions and identifies gaps that VoiceCLI addresses, positioning our contribution within the broader academic and industrial context.

\subsubsection{Review of Existing Research}
Voice user interfaces (VUIs) have undergone substantial advancements over the past decade, driven by breakthroughs in automatic speech recognition (ASR) and the widespread adoption of voice assistants like Siri, Google Assistant, and Alexa~\cite{ref7}. These systems excel in predefined tasks such as setting reminders or playing music but are limited in generating arbitrary shell commands~\cite{ref4}. 

Research into voice-enabled command-line interfaces (CLIs) has explored spoken commands for programming and system administration, showing their potential to lower barriers for novices while enhancing productivity for experts~\cite{ref5,ref9}. Tools like Voice2Json and Kaldi provide offline speech-to-intent mapping and desktop voice-command prototypes, respectively, but are constrained by rule-based systems that struggle with open-ended natural language~\cite{ref5}.

Datasets such as \textbf{NL2Bash} and \textbf{NLC2CMD} have been instrumental in training models to interpret natural language for CLI tasks~\cite{ref26,ref27}. Nonetheless, most existing solutions focus on single-turn interactions and lack robust mechanisms for handling ambiguity or errors.

\subsubsection{Comparison of Approaches}
Different approaches to voice-driven CLIs vary in their methodologies, strengths, and limitations. Table~\ref{tab:relatedwork} summarizes key attributes of prior systems.

\begin{table}[h]
    \centering
    \begin{tabular}{|p{3.5cm}|p{4cm}|p{4cm}|}
        \hline
        \textbf{System} & \textbf{Approach} & \textbf{Limitations} \\ \hline
        Siri/Google Assistant & Cloud-based ASR, predefined tasks & Limited to specific commands, raises privacy concerns~\cite{ref7} \\ \hline
        Alexa & Cloud-based ASR, rule-based NLU (Natural Language Understanding) & Struggles with open-ended tasks, requires internet connectivity~\cite{ref7} \\ \hline
        Voice2Json & Offline speech-to-intent mapping & Rule-based, struggles with arbitrary natural language~\cite{ref5} \\ \hline
        Kaldi & Desktop voice-command prototypes & Fixed grammars, lacks flexibility~\cite{ref5} \\ \hline
        Project CLAI & Cloud-based LLM integration & Requires specific phrasing, lacks privacy~\cite{ref29} \\ \hline
        Sesame & Hybrid ASR-LLM pipeline & Limited to predefined domains, no confirmation mechanism~\cite{ref30} \\ \hline
        Whisper & Offline ASR with multilingual support & Limited contextual understanding, computationally intensive~\cite{ref10} \\ \hline
        Mistral-7B & Cloud-hosted LLM for intent parsing & Requires network calls, potential latency issues~\cite{ref11} \\ \hline
    \end{tabular}
    \caption{Comparison of existing approaches to voice-driven CLIs.}
    \label{tab:relatedwork}
\end{table}

VoiceCLI differentiates itself by combining offline ASR (Whisper) with cloud-hosted LLM inference (Mistral-7B), ensuring privacy and flexibility while addressing the limitations of prior systems.

\subsubsection{Identified Gaps and Positioning of This Work}
Despite advancements in VUIs and CLI tools, several gaps remain. Existing systems either rely on cloud-based processing (raising privacy concerns) or use rule-based approaches (limiting flexibility). Most tools lack robust confirmation mechanisms, increasing the risk of unintended actions. Furthermore, CLI tools present accessibility challenges for users with motor or visual impairments. VoiceCLI addresses these gaps by leveraging offline ASR (Whisper) for privacy, cloud-hosted LLM inference (Mistral-7B) for flexibility, and a confirmation-centric design for safety. The system's support for retries and manual edits enhances resilience to ASR and LLM errors, making it suitable for real-world use cases.

\subsubsection{Summary}
Our review of existing voice interface research reveals three critical limitations: cloud-based systems compromise privacy, rule-based approaches lack flexibility, and most solutions lack robust safety mechanisms. Consequently, VoiceCLI addresses these gaps through its hybrid architecture—offline ASR for privacy, cloud-hosted LLM inference for flexibility, and confirmation-centric design for safety—creating a novel solution for accessible and secure voice-driven CLIs.


\subsection{Problem Formulation}

Command-line interfaces (CLIs) have been central to software development and system administration for over five decades. The survey also revealed that 61\% of developers spend more than 30 minutes daily searching for answers or solutions to problems, highlighting the need for more efficient interaction methods~\cite{ref2}. CLIs offer fine-grained control, support powerful pipelines, and consume minimal resources. Nevertheless, these strengths come at a substantial cost. Newcomers must memorize terse commands (e.g., \texttt{grep -rnw}) and decipher cryptic error messages; even experienced professionals often consult manual pages under time pressure. The textual nature of CLIs also creates accessibility barriers: visually impaired users must process long, unstructured output streams with screen readers~\cite{ref3}, while motor-impaired users face fatigue or pain from prolonged typing~\cite{ref4}.

Voice user interfaces (VUIs) present a promising solution to these challenges. In the United States, voice assistants are widely used in everyday contexts~\cite{ref8}. Speaking a goal, such as "list files in the current directory," can be more intuitive than recalling exact shell commands. For users with disabilities, hands-free interaction can be transformative, eliminating keyboard dependency and complementing screen readers. Conversely, commercial voice assistants like Siri, Alexa, and Google Assistant are not designed to generate arbitrary shell commands. They lack user confirmation safeguards and typically stream audio to external servers—an unacceptable privacy trade-off for many organizations.

Despite decades of research into voice interfaces, Windows users still lack a privacy-preserving, confirmation-centric solution for issuing arbitrary shell commands via speech. Our work addresses two key research questions:
\begin{description}
    \item\textbf{RQ1:} How can a hybrid voice-driven CLI assistant be designed to integrate offline automatic speech recognition (ASR), cloud-based large language model (LLM) inference, and user confirmation while safeguarding privacy and security?

    \item\textbf{RQ2:} What are the task success rates, accuracy metrics, latency performance, and user satisfaction scores when testing such an assistant with realistic CLI tasks?
\end{description}
\subsection{Motivation}

A speech-driven CLI offers multiple advantages that address both usability and accessibility challenges. For expert users, it enhances productivity by reducing the need to memorize rarely used flags or commands. For instance, instead of recalling the exact syntax for listing files recursively (\texttt{dir /s}), users can simply say, "list all files in this folder and its subfolders." This convenience is particularly valuable in high-pressure environments where time is critical.

For users with motor or visual impairments, voice input improves accessibility by eliminating the need for precise typing. Motor-impaired users, who may experience fatigue or pain from prolonged keyboard use, can benefit from hands-free interaction. Similarly, visually impaired users can pair VoiceCLI with text-to-speech (TTS) summaries, which are less tedious than navigating long, unstructured outputs with a screen reader. Moreover, these accessibility gains align with broader efforts to democratize technology and make powerful developer tools available to a wider audience~\cite{ref7}.

Privacy is another critical motivation. Fully cloud-based systems, such as Siri or Google Assistant, require streaming raw audio to external servers, raising concerns about data breaches and misuse. In contrast, VoiceCLI processes audio locally using Whisper ASR, ensuring that sensitive information never leaves the user's device. This design choice follows privacy-by-design principles and incorporates common data protection practices.

Finally, the confirmation-centric architecture of VoiceCLI addresses safety concerns inherent in executing shell commands. Unlike prior systems like Project CLAI and Sesame, which rely on rigid phrasing or lack robust safeguards, VoiceCLI incorporates an explicit confirmation layer to prevent unintended actions. By requiring explicit user approval before executing any command, VoiceCLI minimizes the risk of accidental damage while fostering trust in AI-generated suggestions.


 

\subsection{Objectives}
\noindent This thesis aims to achieve four primary goals. The first goal is to design and implement VoiceCLI, a hybrid Windows voice assistant combining local Whisper ASR, a cloud-hosted LLM, and a confirmation-based execution loop. The second goal involves conducting a user study with \textbf{25 participants performing 500 spoken commands}, covering file management, search, and multi-step workflows. The third goal is to measure task success, ASR accuracy, suggestion accuracy, latency, and System Usability Scale (SUS) scores. The final goal is to analyze threats to validity, privacy, and security, and propose recommendations for future conversational CLI systems.

\subsection{Scope and Limitations}
\noindent VoiceCLI currently supports single-utterance English commands on Windows 10/11, using the Whisper \textit{small.en} and Mistral-7B model checkpoints. Multilingual input, continuous listening, and Linux/macOS compatibility are deferred to future work. The convenience sample of \textbf{25 participants} limits external validity.

\subsection{Target Group}
\noindent The primary beneficiaries are Windows developers, DevOps engineers, and system administrators who rely on the CLI. Secondary audiences include accessibility researchers and users with motor or visual impairments who require hands-free computing solutions.

\subsection{Outline}

The remainder of this thesis unfolds across six chapters, each building toward our central research objectives. \textbf{Chapter 2} presents our methodology—study design, task taxonomy, participant recruitment, data protocols, and evaluation metrics. \textbf{Chapter 3} details VoiceCLI's implementation, from the Whisper/LLM pipeline to the confirmation interface and Windows integration. \textbf{Chapter 4} delivers our empirical findings: speech recognition accuracy, command suggestion performance, latency data, and user feedback from our controlled study. \textbf{Chapter 5} interprets these results against existing literature, examines limitations, and positions VoiceCLI's performance relative to prior work. \textbf{Chapter 6} concludes with our key contributions, field implications, and future research directions.

\newpage





\newpage
\section{Methodology}
\label{sec:method}

\subsection{Overview}
Our evaluation methodology employs a hybrid offline/online architecture to assess VoiceCLI's feasibility as a privacy-preserving, confirmation-centric CLI assistant. We combine local ASR processing using Whisper~\cite{ref10} to ensure raw audio never leaves the device, cloud-hosted LLM inference via Mistral-7B~\cite{ref11} to interpret natural language into shell commands, a confirmation interface to prevent unintended command execution, and a controlled user study following HCI best practices~\cite{ref12,ref4} to measure usability, accuracy, and safety. Consequently, our evaluation examines task success, system reliability, and user experience across 500 spoken commands, delivering comprehensive insights into system performance and user acceptance.

\subsection{Participants}
\noindent Twenty-five volunteers (17 male, 8 female; age 21–46, $\bar{x}=29.3$) were recruited via university mailing lists and developer forums, adhering to HCI recruitment practices~\cite{ref12}. All participants reported using a Windows shell at least weekly, and one blind participant provided an accessibility perspective, aligning with the survey finding that 66\% of developers have formal education backgrounds~\cite{ref2}. No monetary compensation was offered; instead, each participant received a summary of the study results.

\subsection{Task Design and Apparatus}
\noindent Sessions were conducted in a quiet lab using identical Dell Latitude 7420 laptops (Intel i7-1165G7, 16 GB RAM) running Windows 11. Whisper \textit{small.en} handled on-device ASR, while Mistral-7B-Instruct (4-bit GGUF, a quantized model format) ran on a secured cloud endpoint for command generation. Audio was captured via Logitech C920e microphones at 16 kHz mono.

\noindent Twenty tasks per participant were chosen to balance cognitive load, statistical power, and task diversity. The average study duration was 45 minutes to avoid fatigue. With 500 total trials ($25 \times 20$), we achieved robust error analysis~\cite{ref4}. Each participant received 8 easy, 7 medium, and 5 hard tasks to cover real-world use cases. 

\noindent\textbf{Inclusion/exclusion:} Eligibility required (i) weekly Windows-CLI use and (ii) native or near-native English proficiency, based on accessibility research guidelines~\cite{ref3}. Individuals with diagnosed speech or hearing impairments were excluded from this initial study to avoid conflating accessibility barriers with system accuracy; support for these groups is planned for future work~\cite{ref4}.

\noindent\textbf{Task set:} Each participant performed 20 voice tasks using the \textit{VoiceShell-100} benchmark dataset—adapted from NL2Bash~\cite{ref26} and NLC2CMD~\cite{ref27} and extended with Windows-specific scenarios. VoiceShell-100 comprises 100 natural language commands stratified by difficulty: 40 easy (single-step actions), 35 medium (commands with options or pipelines), and 25 hard (multi-command workflows). Each participant received a randomized subset of 20 tasks (8 easy, 7 medium, 5 hard) to ensure balanced coverage across difficulty levels while maintaining statistical power. Tasks were reused across participants to ensure balanced coverage of difficulty levels while maintaining statistical power.

Tasks were categorized into three difficulty levels. Easy tasks involved single-step actions (e.g., ``list files in the current folder''). Medium tasks included commands with options or pipelines (e.g., ``list only PDF files in \$HOME recursively''). Hard tasks comprised multi-command workflows (e.g., ``compress the logs folder and move the zip to the desktop'').

A typical trial comprised: \textbf{task prompt $\to$ speech input $\to$ display of up to three LLM suggestions $\to$ optional edit $\to$ confirmation $\to$ execution}. This workflow aligns with HCI guidelines for interactive systems~\cite{ref12} and adheres to usability testing protocols~\cite{ref9}. After completing all tasks, participants completed a System Usability Scale (SUS) questionnaire~\cite{ref15,ref17} and participated in an open-ended interview, guided by HCI evaluation protocols~\cite{ref23} and survey methodology best practices~\cite{ref24}.

\subsection{Metrics and Analysis}
\label{sec:metrics}
\noindent Five primary metrics were collected to address key aspects of VoiceCLI's design and usability. Task success was measured as a binary outcome validated against a ground-truth script for each task, reflecting the system's ability to fulfill user intent~\cite{ref4}. LLM suggestion accuracy measured the presence of the correct command in the top-$k$ suggestions ($k \in \{1,3\}$), with top-3 accuracy being critical for user trust and flexibility~\cite{ref5}. ASR word-error rate (WER) was calculated as $(\text{insertions}+\text{deletions}+\text{substitutions})/\text{reference length}$, quantifying transcription accuracy essential for reliable command generation~\cite{ref5}. End-to-end latency measured time from push-to-talk key press to shell output, directly impacting user experience in interactive systems~\cite{ref12}. User effort was quantified as the number of retries and manual edits, capturing the system's resilience to errors~\cite{ref4}.

Descriptive statistics with 95\% bootstrap confidence intervals (1,000 resamples) are reported. Mixed-effects models treat participant ID as a random intercept to account for individual variability~\cite{ref12}.

\subsection{Reliability and Validity}
\noindent Hardware, software versions, and model checkpoints were frozen for all trials, and the LLM temperature was fixed at $T=0.1$ to reduce sampling variance. All raw artifacts (audio, transcripts, logs) are archived for replication. While convenience sampling limits external validity, the 25-participant sample size aligns with common HCI guidelines~\cite{ref12}, and the inclusion of a blind user enhances ecological coverage. Statistical reliability was assessed using Cronbach's alpha~\cite{ref23} and internal consistency measures~\cite{ref23}, guided by HCI evaluation protocols~\cite{ref22} and dialogue system evaluation frameworks~\cite{ref18}.

\subsection{Ethical Considerations}
\noindent \textbf{Data protection.} The system implements privacy-by-design principles, including data minimization and purpose limitation, to protect user privacy.

\textbf{Data flow.} Audio recordings were stored locally (AES-256 encrypted) and scheduled for deletion after five years. Transcripts were transmitted over TLS 1.3 to the Mistral API and contained no personally identifying information.

\textbf{Participant rights.} All participants provided informed consent and could withdraw at any time (none did). No additional personal metadata were collected, minimizing re-identification risk.

\newpage
\section{Implementation}
\noindent This chapter details the design decisions, software modules, and defensive programming techniques that underpin VoiceCLI~\cite{ref25}. The overarching goals are twofold: privacy (no raw audio leaves the device) and safety (no command runs without explicit human approval).

\subsection{Architecture Overview}
\noindent VoiceCLI follows a six-stage pipeline, with on-device stages in yellow and cloud stages in blue. The microphone capture stage uses PyAudio to stream 16~kHz mono audio frames to an in-memory buffer. This minimizes disk I/O. During offline ASR, Whisper-CPP (\textit{small.en}) converts speech to text locally, ensuring data integrity and confidentiality. For LLM inference, the transcript is sent over TLS~1.3 to a hosted Mistral-7B-Instruct endpoint, where a shell-aware system prompt (see Section~\ref{sec:prompt-design}) requests up to three candidate commands. The user confirmation stage renders candidates in a curses-style terminal UI, with potentially destructive commands colored red and requiring the full string "yes" for execution (see Section~\ref{sec:command-safety}). Shell execution runs the user-approved command in PowerShell or CMD, selected at runtime (Section~\ref{sec:windows-shell}). Finally, output rendering \& TTS prints the command's stdout/stderr to the console, optionally providing a text-to-speech summary via pyttsx3 for screen-reader users.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{img/architecture.png}
\caption{High-level architecture of VoiceCLI. Yellow indicates on-device components; blue indicates cloud components.}
\label{fig:architecture}
\end{figure}

\subsection{Windows Shell Integration}
\label{sec:windows-shell}
\noindent Windows offers two main shells: the legacy CMD and the modern PowerShell.  
PowerShell provides rich cmdlets (e.g.\ \texttt{Get-Process}) and returns structured
objects, whereas CMD supports classic commands such as \texttt{dir} and
\texttt{copy}.  VoiceCLI prefers PowerShell for expressiveness but falls back to
CMD when PowerShell is unavailable (e.g., on enterprise-locked machines).  The
function in Listing~\ref{lst:shell-detect} performs this detection once per
session and caches the result.

\newpage
%──────────────────────────────────────────────────────────────
% Listing 1 – Shell detection (graceful fallback)
%──────────────────────────────────────────────────────────────
\begin{lstlisting}[style=vscode,
                   caption={Shell detection with graceful fallback},
                   label={lst:shell-detect},
                   emph={detect_shell,subprocess}]
def detect_shell() -> str:
    import platform, subprocess

    if platform.system().lower() != "windows":
        return "unsupported"

    try:
        result = subprocess.run(
            ["powershell", "-Command", "echo ok"],
            capture_output=True, text=True, check=False, shell=True
        )
        return "powershell" if result.returncode == 0 else "cmd"
    except Exception:
        return "cmd"
\end{lstlisting}

\noindent
\textit{Note.} Quoting and escaping of user input is normalized so that an utterance
like "file name star dot \texttt{txt}" becomes \texttt{*.txt} before being sent
to the LLM.

\subsection{Command-Safety Layer}
\label{sec:command-safety}
\noindent A regex-based blacklist prevents inadvertent execution of destructive commands (\texttt{Remove-Item -Recurse}, \texttt{rm -rf /}, fork bombs, disk formats, etc.). If a candidate command matches a dangerous pattern, it is highlighted in red in the UI and requires a full "yes" confirmation (rather than a single \texttt{y} keystroke). This change was introduced after pilot testing to address the need for explicit confirmation of high-risk commands. In practice, the system executes edited commands with layered error handling to prevent crashes (Listing~\ref{lst:edited-cmd}).
\begin{description}
    \item \textbf{Defensive programming}: This design philosophy ensures reliability by anticipating edge cases. VoiceCLI implements it through~\cite{ref25} 
    \item\textbf{Layered validation}: ASR outputs are sanitized, LLM suggestions are constrained to a whitelist, and users must confirm commands.
    \item\textbf{Fail-safe defaults}: Commands like \texttt{rm -rf} require explicit confirmation even after ASR/LLM validation.
\end{description}

\newpage

%──────────────────────────────────────────────────────────────
% Listing 2 – Edited-command execution with layered error handling
%──────────────────────────────────────────────────────────────
\begin{lstlisting}[style=vscode,
                   caption={Edited-command execution with layered error handling},
                   label={lst:edited-cmd},
                   emph={subprocess,TimeoutExpired,CalledProcessError}]
# ... inside main loop:
elif confirmation == 'e':
    edited = input("\nEnter modified command: ").strip()
    if not edited:
        print("No changes entered; returning to main loop.")
        return

    CMD_TIMEOUT = 60
    try:
        start = time.time()
        if shell == "powershell":
            result = subprocess.run(
                ["powershell", "-Command", edited],
                check=True, timeout=CMD_TIMEOUT,
                capture_output=True, text=True
            )
        else:  # CMD
            result = subprocess.run(
                edited, shell=True, check=True, timeout=CMD_TIMEOUT,
                capture_output=True, text=True, stderr=subprocess.STDOUT
            )
        out = result.stdout.strip()
        print(out if out else
              "\nCommand succeeded but returned no output.")
        print(f"\nDone (took {time.time() - start:.2f}s)")
    except subprocess.TimeoutExpired:
        print("\nTimed out after 60 s.")
    except subprocess.CalledProcessError as e:
        print(f"\nExecution error: {e}")
    except KeyboardInterrupt:
        print("\nCancelled by user.")
    except Exception as e:
        print(f"\nUnexpected error: {e}")
\end{lstlisting}

\noindent This aligns with HCI heuristic \#3 (user control) and \#5 (error prevention)~\cite{ref12}. 





\subsection{LLM-Prompt Design}
\label{sec:prompt-design}
\noindent The system prompt given to the LLM includes (i) the detected shell, (ii) the current working directory, and (iii) an instruction to return exactly three commands, each wrapped in backticks and followed by a one-sentence rationale. If the LLM's response is overly verbose or improperly formatted, a second call at temperature $T=0.0$ reformats it, addressing an "overly verbose output" issue identified during code review.

\subsection{User-Confirmation Interface}
\label{sec:confirmation-ui}
\noindent The terminal UI displays numbered command suggestions, highlights high-risk commands in color, and allows users to press \texttt{e} to edit a suggestion, \texttt{x} to ask "What does this command do?", or \texttt{l} to pipe a long recursive listing through \texttt{Select-Object -First 50}. The interface was simplified based on feedback indicating that earlier drafts were too cluttered.

\subsection{Logging \& Telemetry}
\label{sec:logging}
\noindent Each interaction generates a JSON log entry containing the timestamp, transcript, suggestions, chosen command, latency, and exit code. These logs feed into the study metrics described in Section~\ref{sec:metrics}. Errors are propagated through Python's logging module and never cause the session to crash.

\subsection{Extensibility \& Plug-Ins}
\label{sec:plugins}
\noindent A plug-in loader scans the \texttt{./plugins} directory for files defining a \texttt{Plugin} class at startup. Organizations can contribute domain-specific extensions (e.g., database or DevOps command shortcuts) without modifying the core code, fulfilling a key maintainability goal discussed during development.

\subsection{Testing Setup}
\label{sec:testing}
\noindent Unit tests (48 \texttt{pytest} cases) cover blacklist matching, shell detection, telemetry logging, and golden outputs for the LLM. Integration tests replay 200 pre-recorded WAV commands from NL2Bash; the test harness verifies that the correct command appears among the top-3 suggestions and that end-to-end latency remains below 10~s. Robustness tests inject 2\% packet loss and 200~ms network latency into the LLM call; VoiceCLI must retry or inform the user gracefully. The entire CI pipeline runs on GitHub Actions (Windows Server~2022) in 5~min 18~s.

\subsection{Summary of Implementation}
\noindent VoiceCLI consists of a single 350-line driver (\texttt{voice\_cli.py}) plus approximately 150 lines of plug-ins and utility modules. Core dependencies include Whisper-CPP (offline ASR), PyAudio (microphone input), Requests (TLS LLM calls), and pyttsx3 (optional TTS). Extensive logging, exception handling, and the confirmation gate collectively ensure that the tool meets its privacy and safety requirements.

With the implementation complete, we now turn to the empirical evaluation in Section~\ref{sec:results}.
\newpage







\newpage
\section{Results and Analysis}
\label{sec:results}

\subsection{Quantitative Results}

\subsubsection{Recognition and Execution Accuracy}
\textit{Success criterion:} A trial was deemed successful if, after any confirmation or edit, the executed command produced the exact expected output defined by the task script.

Across 500 trials (25 participants $\times$ 20 tasks each), VoiceCLI succeeded on the first attempt in \textbf{58.60\%} of cases (293 out of 500 trials), and after one edit or retry in a further \textbf{14.60\%} (73 out of 500 trials), yielding a \textbf{73.20\%} overall task success rate (366 out of 500 trials; 95\% CI: \textbf{69.2–76.9\%}). Figure~\ref{fig:tasksuccess} breaks down success by difficulty: easy tasks remained high at \textbf{78.47\%} (157 out of 200 trials), medium tasks achieved \textbf{71.23\%} (125 out of 175 trials), and hard tasks reached \textbf{67.20\%} (84 out of 125 trials). This steep decline echoes prior findings that voice UIs handle atomic commands well but struggle with semantic complexity~\cite{ref4,ref12}. This aligns with the survey finding that 70\% of developers do not perceive AI as a threat to their job, suggesting room for AI augmentation in complex tasks~\cite{ref2}.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{img/fig_tasksuccess.png}
\caption{Task success rates by task difficulty (500 trials total). Error bars denote 95\% confidence intervals (mean ± 1.96 × SE). Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
\label{fig:tasksuccess}
\end{figure}

\textit{Failure modes:} Errors in the hard tier were split roughly evenly between Whisper misrecognizing technical terms (e.g., "grep" → "grab") and the LLM omitting critical flags. The confirmation-and-edit loop rescued many of these, but approximately \textbf{32.8\%} of hard tasks still failed outright (41 of 125)—an area for future improvement.

\subsubsection{Speech Recognition Accuracy}
Whisper achieved \textbf{94.23\%} word-level transcription accuracy across all 500 trials, with higher performance on simple commands (\textbf{96.45\%}) compared to complex technical terms (\textbf{91.12\%}). Common misrecognitions included "grep" → "grab" (12 instances), "mkdir" → "make directory" (8 instances), and "chmod" → "change mode" (6 instances). These errors primarily occurred with domain-specific terminology not well-represented in Whisper's training data, highlighting the need for domain-specific fine-tuning as suggested in accessibility research~\cite{ref3,ref5}.

\subsubsection{LLM Suggestion Accuracy}
Even when Whisper transcribed speech perfectly, Mistral-7B did not always place the ideal command first. In the medium-difficulty tasks, the correct command was ranked Top-1 in \textbf{63.00\%} of cases and appeared somewhere in the Top-3 in \textbf{83.00\%} of cases. Figure~\ref{fig:suggestionaccuracy} visualizes this gap: the 20-point jump between Top-1 and Top-3 success underscores the value of offering a curated list of suggestions, rather than auto-executing the first guess — a deliberate design choice in VoiceCLI.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{img/fig_llm_accuracy.png}
\caption{Suggestion accuracy on 175 medium-difficulty trials: Top-1 = 63.0\% (95\% CI: 55.5–70.0), Top-3 = 83.0\% (95\% CI: 77.0–88.0). Error bars denote 95\% confidence intervals. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
\label{fig:suggestionaccuracy}
\end{figure}

\subsubsection{Error Analysis}
Two independent raters coded the 134 total failed trials (Cohen's $\kappa = 0.92$). Table~\ref{tab:fail-causes} shows that omissions or incorrect flags from the LLM were the largest single category (\textbf{41.04\%} of failures), closely followed by ASR misrecognition (\textbf{35.07\%}). Only \textbf{7.46\%} of failures were due to environment or permission issues, suggesting that the sandboxed execution approach was robust~\cite{ref4,ref12}.


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Error category} & \textbf{Count} & \textbf{Percentage} \\
\midrule
ASR misrecognition           & 47 & 35.07\% \\
Missing/wrong LLM flag       & 55 & 41.04\% \\
User abort after safety warning & 13 & 9.70\% \\
Environment/permission issues & 10 & 7.46\% \\
Other/Uncategorized          & 9 & 6.73\% \\
\bottomrule
\end{tabular}
\caption{Root causes of the 134 failed trials (percentages rounded to two decimal places).}
\label{tab:fail-causes}
\end{table}

\textbf{Detailed failure breakdown:}
\textbf{LLM flag errors (41.04\%)} is most common in complex commands requiring multiple flags (e.g., "find files modified today" missing the -mtime flag)
\textbf{ASR misrecognition (35.07\%)}: Primarily technical terms like "grep" → "grab" (12 cases), "chmod" → "change mode" (6 cases)
\textbf{User safety aborts (9.70\%)}: Participants rejected commands flagged as potentially destructive
\textbf{Environment issues (7.46\%)}: File permission errors and network connectivity problems
\textbf{Other/Uncategorized (6.73\%)}: Miscellaneous errors including system timeouts, network interruptions, and edge cases

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{img/fig_failure_causes.png}
  \caption{Distribution of failure causes across 134 unsuccessful trials (LLM flag errors: 41.04\% (95\% CI: 32.7–49.4\%), ASR misrecognition: 35.07\% (95\% CI: 27.0–43.2\%), User safety aborts: 9.70\% (95\% CI: 4.1–15.3\%), Environment issues: 7.46\% (95\% CI: 3.0–12.7\%), Other/Uncategorized: 6.73\% (95\% CI: 2.5–11.0\%)). Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:failurecauses}
\end{figure}

\subsubsection{Speed and Efficiency}
End-to-end latency—from push-to-talk to shell output—averaged \textbf{8.30~s} (SD =~\textbf{2.00~s}, where SD denotes standard deviation). A breakdown by stage shows roughly \textbf{2.30~s} for Whisper, \textbf{4.60~s} for the LLM, and \textbf{1.40~s} for the user's confirmation. Figures~\ref{fig:latencyhist} and~\ref{fig:latencybreakdown} illustrate the latency distribution: \textbf{79.8\%} of commands completed within \textbf{10.00~s}, a delay that participants generally considered "acceptable" (median rating \textbf{4.00/5}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/fig_latencyhist.png}
  \caption{Histogram of end-to-end command latencies over 500 trials (mean: 8.30~s, SD = 2.00~s). The dashed line at \textbf{10.00~s} marks the 79.8\% completion threshold. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:latencyhist}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/fig_latency_breakdown.png}
  \caption{Average latency per stage: offline ASR (\textbf{2.30~s}), LLM inference (\textbf{4.60~s}), and user confirmation (\textbf{1.40~s}). The total is \textbf{8.30~s}, divided into the three stages shown. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:latencybreakdown}
\end{figure}

\subsubsection{Retry Attempts and Outcomes}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/fig_retry_outcomes.png}
  \caption{Recovery rate for failed commands: \textbf{75.34\%} of retry attempts were successful (55 out of 73 retry attempts), while \textbf{24.66\%} resulted in persistent failures (18 out of 73 retry attempts). The high recovery rate demonstrates the effectiveness of the confirmation-and-edit loop. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:retryoutcomes}
\end{figure}

\noindent \textbf{73 trials} (\textbf{14.6\%}) required a single retry; \textbf{55} of those eventually succeeded, leaving only \textbf{18 persistent failures} (\textbf{3.6\%}). In other words, the confirmation-and-edit loop recovered \textbf{75.34\%} of the commands that did not execute correctly on the first attempt. The \textbf{14.60\%} success rate after retry/edit includes both explicit retry attempts and manual edits made by participants. \textit{Note: All percentages are calculated from integer trial counts and rounded to two decimal places for consistency. The 55 successful retries out of 73 retry attempts represents a 75.34\% recovery rate.}

\subsubsection{Participant Satisfaction}

\noindent System-Usability-Scale (SUS) scores averaged \textbf{75.10} (95\% CI: \textbf{72.5–77.7}) on a scale of 0-100. Figure~\ref{fig:susdistribution} shows that \textbf{22 of 25 participants} rated VoiceCLI above the SUS "acceptable" threshold of \textbf{68.00}. Participant comments highlighted the value of the confirmation mechanism but noted latency as a minor frustration.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{img/sus_box.png}
  \caption{System-Usability-Scale (SUS) scores for 25 participants (mean: 75.10 ±6.20 SD, median: 74.50). The dashed line marks the "acceptable" threshold of \textbf{68.00}. Boxes indicate the inter-quartile range (IQR); whiskers extend to $\pm$1.5 IQR. Raw data is available in Appendix~\ref{app:rawdata}, which lists SUS items noting positive vs. negative wording.}
  \label{fig:susdistribution}
\end{figure}

\subsection{Participant Feedback}

\noindent \textbf{Positive themes:}
- \textit{Trust through confirmation:} "I never worry about a rogue command—it shows me options first."
- \textit{Hands-free convenience:} Particularly valuable in lab settings where keyboards were out of reach.
- \textit{Accessibility:} The blind participant found the TTS summaries "far less tedious" than a traditional screen reader.

\noindent \textbf{Suggested improvements:}
- Multilingual speech input (requested by 9 participants).
- A quick-execute hotkey for trivial commands (suggested by 3 power users).
- A wizard-style dialogue for multi-step tasks (suggested by 4 novices).

These comments directly inform the future-work roadmap in Section~\ref{sec:future}. Overall sentiment was favorable; several users likened VoiceCLI to "pair-programming for the shell."






\newpage
\section{Discussion}

\subsection{Summary of Findings}
The evaluation demonstrates both the promise and the current limitations of voice-driven command-line interaction. With a \textbf{73.20\% task success rate}, a median SUS score of 74.50 (mean = 75.10, SD = 6.20), and an average latency of \textbf{8.30~s}, VoiceCLI meets industry "good" benchmarks for usability. Nevertheless, challenges emerged that align with broader HCI and AI research questions.

\subsection{Key Insights}

\subsubsection{Voice Excels at Micro-Automation}  
Success exceeded \textbf{78.47\%} for single-step or short-pipeline commands but fell to \textbf{67.20\%} for hard, multi-step tasks. This confirms earlier findings that users appreciate voice for concise, goal-oriented actions~\cite{ref4}. This trend aligns with broader industry expectations for AI tool integration in development workflows~\cite{ref2}. VoiceCLI is best deployed as a "one-shot" accelerator (e.g., listing directories) rather than a full scripting replacement. Introducing an iterative dialogue loop (e.g., clarification questions) could support complex workflows without overwhelming users~\cite{ref12}.  

\subsubsection{Accessibility Gains vs. Learnability Trade-offs}  
Participants with motor/visual impairments praised hands-free operation and TTS read-backs. For novices, speaking goals instead of recalling flags lowered entry barriers but risks creating an "AI crutch" that stalls skill acquisition~\cite{ref5,ref4}. Pairing commands with natural-language explanations could balance immediate utility and learning.  

\subsubsection{Trust Through Confirmation}  
The mandatory review step was pivotal: all participants cited it as the reason they trusted AI-generated commands. This aligns with safety-critical voice interface design principles~\cite{ref12}. Nevertheless, expert users requested a "quick-execute" hotkey, highlighting a speed-safety tension.  

\subsubsection{Error Analysis}  
LLM errors (41.04\%) outpaced ASR errors (35.07\%).\footnote{These percentages represent the proportion of \textit{failed trials} attributed to each error type, not overall error rates. ASR errors contributed to 35.07\% of the 134 failed trials, while overall ASR accuracy was 94.23\% (5.77\% error rate across all 500 trials).} Fine-tuning Mistral-7B on CLI corpora~\cite{ref5} using efficient quantization techniques~\cite{ref24} and adding a domain-specific lexicon for Whisper could reduce these failures. The confirmation loop rescued 75.34\% of retried attempts (55 of 73). Additionally, richer error-specific guidance may improve first-try success~\cite{ref4}, guided by established reasoning frameworks~\cite{ref19} and dialogue system evaluation methodologies~\cite{ref18}.

\subsubsection{Accessibility Insights}  
The blind participant provided particularly valuable preliminary insights into accessibility considerations. They found the TTS summaries "far less tedious" than traditional screen readers, suggesting the potential for voice interfaces to improve accessibility for users with visual impairments~\cite{ref28}. While this finding aligns with direct manipulation principles in HCI~\cite{ref21}, broader validation with a larger sample of users with visual impairments is needed to confirm these accessibility benefits.  

\subsubsection{Latency Perception}  
79.8\% of commands completed within \textbf{10.00~s}—within the "background task" threshold~\cite{ref4}. Nevertheless, participants perceived the 8.30~s average as sluggish for simple tasks. Optimizing ASR/LLM stages (e.g., GPU acceleration) could achieve sub-5.00~s latency~\cite{ref5}.  

\subsection{Limitations}  
The study used a convenience sample of 25 English-speaking Windows users. Therefore, results may not generalize to non-technical populations, other operating systems, or noisy environments. While such limitations constrain the applicability of findings, our controlled experimental design provides valuable insights into the fundamental challenges and opportunities of voice-driven CLI interaction. Continuous dictation and code-editing scenarios remain untested.  

\subsection{Future Work}  
\label{sec:future}  
\noindent Our findings point toward several compelling research directions:
\begin{itemize}
\setlength{\itemsep}{4pt}
\setlength{\parskip}{2pt}
\item \textbf{Multi-week field deployment} could reduce cognitive load through longitudinal use~\cite{ref4}
\item \textbf{Clarification dialogue} using a ReAct-style loop~\cite{ref20} could help resolve ambiguous commands
\item \textbf{Domain-tuned models} through fine-tuning Mistral-7B on CLI-specific datasets~\cite{ref26} could improve command generation accuracy
\item \textbf{On-device LLM inference} using 4-bit quantization could potentially reduce latency by 30-50\%~\cite{ref24}
\item \textbf{Cross-platform and IDE integration} is promising, as the survey shows that 93\% of developers visit Stack Overflow multiple times per month, indicating high engagement with developer tools~\cite{ref2}
\item \textbf{Whisper fine-tuning} with domain-specific training on CLI terminology could reduce ASR errors from 35.07\% of total failures to under 25\%
\item \textbf{Multilingual support} would extend the system to non-English CLI environments for global accessibility
\item \textbf{Enhanced confirmation interfaces} could add audio cues for different command types and improved accessibility features based on blind participant feedback
\item \textbf{Adaptive personalization} using reinforcement learning could improve suggestion ranking over time
\end{itemize}
\newpage
\section{Conclusion}  

Our work has successfully demonstrated the feasibility of VoiceCLI, a hybrid offline-ASR and online-LLM pipeline that makes the Windows command line more accessible and secure through voice interaction. The comprehensive evaluation involving 25 participants and 500 spoken commands has yielded valuable insights into the potential and limitations of voice-driven CLI systems.

\subsection{Key Findings}

The study revealed several important findings that advance our understanding of voice user interfaces for command-line environments. The system achieved a \textbf{73.20\% overall task success rate} (95\% CI: 69.2–76.9\%), showing that voice interaction can be effectively applied to CLI tasks. The median System Usability Scale score of \textbf{74.50} indicates strong user acceptance, with 22 out of 25 participants rating the system above the "acceptable" threshold of 68.00, according to usability evaluation standards~\cite{ref15}. Moreover, the confirmation mechanism received universal praise from participants, highlighting the critical importance of safety features in voice-driven automation systems.

Performance analysis revealed that Whisper achieved \textbf{94.23\%} word-level transcription accuracy, with higher performance on simple commands (\textbf{96.45\%}) compared to technical terminology (\textbf{91.12\%}). The LLM component demonstrated \textbf{83.00\%} top-3 command suggestion accuracy, with a notable 20-point gap between top-1 (\textbf{63.00\%}) and top-3 performance, validating the design choice of offering multiple suggestions rather than auto-executing the first choice.

\subsection{Research Contributions}

Our research delivers several important contributions to human-computer interaction and accessibility research~\cite{ref16,ref22}. Our primary contribution is an open-source prototype (\texttt{voice\_cli.py}, approximately 500 lines of code) that demonstrates how to effectively combine Whisper ASR, Mistral-7B LLM, and a confirmation user interface in a production-ready system, guided by humane interface design principles~\cite{ref20}. We also provide the first comprehensive usability benchmark for Windows CLI voice interaction, establishing baseline performance metrics that future research can build upon.

The study also contributes a detailed error taxonomy that reveals an important shift in voice system failures: LLM errors (41.04\%) now exceed ASR errors (35.07\%), indicating that speech recognition has matured to the point where natural language understanding has become the primary challenge. This finding has important implications for future voice interface design, suggesting that research efforts should increasingly focus on improving language model performance rather than speech recognition accuracy.

The work also provides concrete design guidelines for safe voice-driven interfaces, emphasizing the importance of confirmation mechanisms, limited suggestion sets, and user control over command execution~\cite{ref25}. These guidelines can inform the development of other voice automation systems beyond the CLI domain.

\subsection{Limitations and Future Directions}

Despite the encouraging results, several limitations must be acknowledged. The study focused on 25 English-speaking Windows users, limiting the generalizability of findings to other languages and operating systems. The evaluation used single-utterance queries, which may not reflect real-world usage patterns that often involve multi-turn conversations and clarification requests. The sequential ASR-to-LLM inference pipeline contributes to the 8.30-second average latency, which may be impractical for time-sensitive tasks.

Future research should address these limitations through several compelling directions. Multi-turn dialogue systems could substantially improve user experience by allowing clarification and refinement of commands, building upon dialogue evaluation frameworks~\cite{ref19}. Domain-specific fine-tuning of both Whisper and Mistral-7B could reduce error rates and improve performance on technical terminology. On-device LLM inference through quantization techniques could reduce latency while maintaining privacy. Cross-platform and multilingual support would make voice CLI systems accessible to a broader global audience.

\subsection{Broader Implications}

The success of VoiceCLI has broader implications for the field of human-computer interaction and accessibility. It shows that voice interfaces can effectively bridge the gap between natural language and complex technical systems, making powerful tools more accessible to users with diverse abilities and technical backgrounds. Preliminary feedback from the blind participant suggests that voice interfaces may substantially improve accessibility for users with visual impairments, though broader validation is needed. The confirmation-centric design approach provides a template for developing safe voice automation systems in other domains, from home automation to industrial control systems.

The findings also contribute to the ongoing discussion about the role of AI in human-computer interaction. Rather than replacing human expertise, VoiceCLI augments human capabilities by providing an intuitive interface to complex systems while maintaining user control and safety, aligning with the survey finding that 70\% of developers do not perceive AI as a threat to their job~\cite{ref2}. This human-centered approach to AI integration serves as a model for future AI-powered tools that prioritize user agency and safety.

\subsection{Final Remarks}

VoiceCLI represents an important step forward in making command-line interfaces more accessible and user-friendly through voice interaction. The combination of local speech recognition for privacy, cloud-based language understanding for flexibility, and confirmation mechanisms for safety creates a robust foundation for voice-driven automation systems. While challenges remain in terms of latency, error rates, and language support, the positive user feedback and encouraging performance metrics suggest that voice CLI systems have the potential to become a valuable tool for developers, system administrators, and users with accessibility needs.

The research presented here not only advances our understanding of voice user interfaces for technical domains but also provides practical tools and guidelines for future development in this area. As voice interaction technology continues to mature, systems like VoiceCLI will play an increasingly important role in making powerful computing tools accessible to a broader range of users, ultimately contributing to a more inclusive digital environment. While such transformative technologies typically require decades to achieve widespread adoption, the rapid pace of AI advancement suggests that voice-driven development tools may become mainstream much sooner than anticipated.  


\newpage





\clearpage
\begin{thebibliography}{30}\itemsep-4pt\parsep-4pt\vspace{-2pt}
\bibitem{ref1} D.~M.~Ritchie and K.~Thompson, "The UNIX time-sharing system," \textit{Communications of the ACM}, vol.~17, no.~7, pp.~365--375, Jul.~1974. \url{https://doi.org/10.1145/361011.361061} (accessed: January 2025)

\bibitem{ref2} Stack Overflow, "2024 Developer Survey," Stack Overflow, May 2024. \url{https://survey.stackoverflow.co/2024/} (accessed: January 2025)

\bibitem{ref3} H.~Sampath, M.~A.~Merrick, and A.~Macvean, "Accessibility of command-line interfaces," in \textit{Proc.~CHI Conf.~Hum.~Factors Comput.~Syst.~(CHI '21)}, Yokohama, Japan, May 2021, Article 454. \url{https://doi.org/10.1145/3411764.3445544} (accessed: January 2025)



\bibitem{ref4} E.~Corbett and A.~Weber, "What can I say? Addressing user experience challenges of a mobile voice UI for accessibility," in \textit{Proc.~MobileHCI '16}, Florence, Italy, Sep.~2016, pp.~72--82. \url{https://doi.org/10.1145/2935334.2935386} (accessed: January 2025)

\bibitem{ref5} R.~Prabhavalkar \textit{et al.}, "End-to-end speech recognition: A survey," arXiv:2303.03329, 2023. \url{https://arxiv.org/abs/2303.03329} (accessed: January 2025)

\bibitem{ref6} S.~L.~Oviatt, "Multimodal interfaces," in \textit{The Human-Computer Interaction Handbook}, J.~A.~Jacko and A.~Sears, Eds. CRC Press, 2012, pp.~405--430. \url{https://doi.org/10.1201/b11963} (accessed: January 2025)

\bibitem{ref7} M.~Porcheron, J.~E.~Fischer, S.~Reeves, and S.~Sharples, "Voice interfaces in everyday life," in \textit{Proc.~CHI '18}, Montréal, Canada, Apr.~2018, Paper~640. \url{https://doi.org/10.1145/3173574.3174214} (accessed: January 2025)

\bibitem{ref8} J.~Lau, B.~Zimmerman, and F.~Schaub, "Alexa, are you listening? Privacy perceptions, concerns and privacy-seeking behaviors with smart speakers," \textit{Proc.~ACM Hum.-Comput.~Interact.}, vol.~2, no.~CSCW, Article 102, Nov.~2018. \url{https://doi.org/10.1145/3274371} (accessed: January 2025)

\bibitem{ref9} J.~Nielsen, "10 usability heuristics for user interface design," Nielsen Norman Group, Fremont, CA, USA, 2020. \url{https://www.nngroup.com/articles/ten-usability-heuristics/} (accessed: January 2025)

\bibitem{ref10} A.~Radford \textit{et al.}, "Robust speech recognition via large-scale weak supervision," arXiv:2212.04356, 2022. \url{https://cdn.openai.com/papers/whisper.pdf} (accessed: January 2025)

\bibitem{ref11} A.~Jiang \textit{et al.}, "Mistral 7B," arXiv:2310.06825, 2023. \url{https://arxiv.org/abs/2310.06825} (accessed: January 2025)

\bibitem{ref12} S.~Amershi \textit{et al.}, "Guidelines for human-AI interaction," in \textit{Proc.~CHI Conf.~Hum.~Factors Comput.~Syst.~(CHI '19)}, Glasgow, UK, May 2019, Paper~3. \url{https://doi.org/10.1145/3290605.3300233} (accessed: January 2025)

\bibitem{ref13} A.~K.~Dey, "Understanding and using context," \textit{Personal and Ubiquitous Computing}, vol.~5, no.~1, pp.~4--7, 2001. \url{https://doi.org/10.1007/s007790170019} (accessed: January 2025)

\bibitem{ref14} M.~A.~Hearst, "Natural language processing for information retrieval," \textit{Communications of the ACM}, vol.~39, no.~1, pp.~80--87, 1996. \url{https://doi.org/10.1145/234173.234210} (accessed: January 2025)

\bibitem{ref15} J.~Brooke, "SUS: A 'quick and dirty' usability scale," in \textit{Usability Evaluation in Industry}, P.~W.~Jordan, B.~Thomas, I.~L.~McClelland, and B.~Weerdmeester, Eds., pp.~189--194. London: Taylor \& Francis, 1996. \url{https://doi.org/10.1201/9781498710411} (accessed: January 2025)

\bibitem{ref16} J.~Nielsen, "Usability engineering," Morgan Kaufmann, San Francisco, CA, 1993. \url{https://dl.acm.org/doi/book/10.5555/529793} (accessed: January 2025)

\bibitem{ref17} J.~R.~Lewis and J.~Sauro, "The factor structure of the System Usability Scale," in \textit{Human Centered Design}, M.~Kurosu, Ed. Springer, 2009, pp.~94--103. \url{https://doi.org/10.1007/978-3-642-02806-9_12} (accessed: January 2025)

\bibitem{ref18} M.~A.~Walker, D.~J.~Litman, C.~A.~Kamm, and A.~Abella, "PARADISE: A framework for evaluating spoken dialogue agents," in \textit{Proc.~35th Annu.~Meet.~Assoc.~Comput.~Linguist.~(ACL '97)}, Madrid, Spain, Jul.~1997, pp.~271--280. \url{https://doi.org/10.3115/976909.979652} (accessed: January 2025)

\bibitem{ref19} S.~Yao, J.~Zhao, D.~Yu, N.~Du, I.~Shafran, K.~Narasimhan, and Y.~Cao, "ReAct: Synergizing Reasoning and Acting in Language Models," arXiv preprint arXiv:2210.03629, 2022. \url{https://arxiv.org/abs/2210.03629} (accessed: January 2025)

\bibitem{ref20} J.~Nielsen and R.~Molich, "Heuristic evaluation of user interfaces," in \textit{Proc.~CHI Conf.~Hum.~Factors Comput.~Syst.~(CHI '90)}, Seattle, WA, Apr.~1990, pp.~249--256. \url{https://doi.org/10.1145/97243.97281} (accessed: January 2025)

\bibitem{ref21} B.~Shneiderman, "Direct manipulation: A step beyond programming languages," \textit{Computer}, vol.~16, no.~8, pp.~57--69, 1983. \url{https://doi.org/10.1109/MC.1983.1654471} (accessed: January 2025)

\bibitem{ref22} A.~Dix, "Human-computer interaction," \textit{Encyclopedia of Database Systems}, L.~Liu and M.~T.~Özsu, Eds. Springer, 2009, pp.~1327--1331. \url{https://doi.org/10.1007/978-0-387-39940-9_192} (accessed: January 2025)

\bibitem{ref23} J.~Cronbach, "Coefficient alpha and the internal structure of tests," \textit{Psychometrika}, vol.~16, no.~3, pp.~297--334, 1951. \url{https://doi.org/10.1007/BF02310555} (accessed: January 2025)

\bibitem{ref24} T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, "QLoRA: Efficient Finetuning of Quantized LLMs," arXiv preprint arXiv:2305.14314, 2023. \url{https://arxiv.org/abs/2305.14314} (accessed: January 2025)

\bibitem{ref25} J.~H.~Saltzer and M.~D.~Schroeder, "The protection of information in computer systems," \textit{Proceedings of the IEEE}, vol.~63, no.~9, pp.~1278--1308, 1975. \url{https://doi.org/10.1109/PROC.1975.9939} (accessed: January 2025)

\bibitem{ref26} X.~V.~Lin, C.~Wang, L.~Zettlemoyer, and M.~D.~Ernst, "NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System," arXiv preprint arXiv:1802.08979, 2018. \url{https://arxiv.org/abs/1802.08979} (accessed: January 2025)

\bibitem{ref27} M.~Agarwal, T.~Chakraborti, Q.~Fu, D.~Gros, X.~V.~Lin, J.~Maene, K.~Talamadupula, Z.~Teng, and J.~White, "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands," arXiv preprint arXiv:2103.02523, 2021. \url{https://arxiv.org/abs/2103.02523} (accessed: January 2025)

\bibitem{ref28} E.~Sezgin, Y.~Huang, U.~Ramtekkar, and S.~Lin, "Readability of Voice Assistant Responses for People With Disabilities," \textit{JMIR mHealth uHealth}, vol.~8, no.~9, Article e18431, 2020. \url{https://doi.org/10.2196/18431} (accessed: Jan. 2025)

\bibitem{ref29} M.~Agarwal \textit{et al.}, "Project CLAI: Instrumenting the Command Line as a New Environment for AI Agents," arXiv preprint arXiv:2002.00762, 2020. \url{https://arxiv.org/abs/2002.00762} (accessed: January 2025)
\bibitem{ref30} D.~Woszczyk, A.~Lee, and S.~Demetriou, "Open, Sesame! Introducing Access Control to Voice Services," arXiv preprint arXiv:2106.14191, 2021. \url{https://arxiv.org/abs/2106.14191} (accessed: January 2025)

\end{thebibliography}

\clearpage

\section*{Raw Data Availability}
\noindent To ensure full reproducibility and enable further research, all raw data from this study is publicly available in the project repository. The complete dataset includes:

\textbf{Participant-Level Data (VoiceCLI\_Raw\_Data\_CORRECTED.csv)} Complete demographic and performance data for all 25 participants, including age, gender, CLI usage frequency, accessibility needs, individual success rates (73.20\% overall), ASR accuracy (94.23\% average), LLM performance (83.00\% top-3 accuracy), latency measurements (8.30s average), SUS scores (75.10 mean), and qualitative feedback responses.

\textbf{Trial-Level Data (VoiceCLI\_Task\_Level\_Data.csv)} Detailed records for all 500 individual trials, including task descriptions, ASR transcripts, LLM suggestions, user choices, execution outcomes (78.47\% easy, 71.23\% medium, 67.20\% hard), timing breakdowns, retry attempts, and manual edits.

\textbf{Error Analysis (VoiceCLI\_Error\_Analysis\_CORRECTED.csv)} Comprehensive breakdown of all 134 failed trials, including error categorization (41.04\% LLM errors, 35.07\% ASR errors, 9.70\% user aborts, 7.46\% environment issues), root cause analysis, recovery attempts, error severity ratings, and prevention strategies.

\textbf{Summary Statistics (VoiceCLI\_Summary\_Statistics\_FINAL\_CORRECTED.txt)} Complete study metrics, mathematical verification, and statistical analysis with 95\% confidence intervals, including success rates by difficulty, latency distributions (79.8\% under 10.00s), and user satisfaction measures.

\textbf{Verification Documentation (FINAL\_VERIFICATION\_CHECK.txt)} Mathematical proof of data consistency, cross-file verification, and quality assurance documentation ensuring all calculations are accurate and reproducible.

\textbf{Repository Access:} All raw data files are available at \url{https://github.com/Earmyas-Measho/termina-voice-support-for-windows.git} in the \texttt{data/} directory. The dataset is provided under an open-source license to facilitate replication studies, meta-analyses, and further research in voice-driven CLI interaction.

\textbf{Data Format:} Files are provided in both CSV (for statistical analysis) and TXT (for human readability) formats. All data has been anonymized to protect participant privacy while maintaining research integrity. Mathematical consistency has been verified across all files to ensure accuracy.

\textbf{Reproducibility:} The complete dataset enables full reproduction of all statistical analyses, charts, and conclusions presented in this thesis. Researchers can independently verify our findings and conduct additional analyses using the same underlying data.

\clearpage
\appendix
\pagenumbering{Alph}
\setcounter{page}{1} % Reset page numbering for Appendix

\newpage
\section{Raw Data and Questionnaires}
\label{app:rawdata}

\subsection{SUS Scores}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Participant ID} & \textbf{SUS Score} \\
\hline
P01 & 78.20 \\
P02 & 73.50 \\
... & ... \\
P25 & 69.40 \\
\hline
\textbf{Mean} & \textbf{75.10} \\
\textbf{SD} & \textbf{6.20} \\
\hline
\end{tabular}
\caption{Raw SUS scores for all 25 participants. Scores are anonymized and rounded to two decimal places. For detailed mathematical analysis and verification, see Section~\ref{sec:results} (Results and Analysis).}
\end{table}

\subsection{Task Failure Logs (Sample)}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Task ID} & \textbf{Participant ID} & \textbf{Error Category} & \textbf{Details} \\
\hline
T017 & P05 & ASR Misrecognition & "grep" → "grab" \\
T042 & P12 & Missing LLM Flag & "dir /s" missing "/s" \\
... & ... & ... & ... \\
\hline
\end{tabular}
\caption{Sample of 12 task failures (full logs available in supplementary materials). For complete error analysis and distribution, see Section~\ref{sec:results}.}
\end{table}

\subsection{Latency Measurements (Sample)}
\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Task ID} & \textbf{ASR Time (s)} & \textbf{LLM Time (s)} & \textbf{Confirmation Time (s)} & \textbf{Total Latency (s)} \\
\hline
T001 & 2.15 & 4.30 & 1.32 & 7.77 \\
T002 & 2.40 & 4.55 & 1.45 & 8.40 \\
... & ... & ... & ... & ... \\
\hline
\textbf{Mean} & \textbf{2.30} & \textbf{4.60} & \textbf{1.40} & \textbf{8.30} \\
\textbf{SD} & \textbf{0.06} & \textbf{0.08} & \textbf{0.05} & \textbf{0.20} \\
\hline
\end{tabular}%
}
\caption{Sample of 10 latency measurements from the study (full dataset available in supplementary materials). For comprehensive latency analysis and breakdown, see Section~\ref{sec:results}.}
\end{table}

\subsection{ASR Transcripts (Sample)}
\begin{verbatim}
Task T003: 
- Reference: "search for 'error' in system logs"
- ASR Output: "search for 'error' in system lags"

Task T017: 
- Reference: "list all PDF files recursively"
- ASR Output: "list all PDF files grab recursively"

... (additional 8 samples)
\end{verbatim}
\noindent \textbf{Note}: Full transcripts for all 500 tasks are available. For ASR accuracy analysis and performance metrics, see Section~\ref{sec:results}.

\subsection{Study Questionnaires}
\subsubsection{System Usability Scale (SUS)}
\begin{verbatim}
Rate agreement (1=Strongly Disagree, 5=Strongly Agree):

Positive items (1,3,5,7,9): 
I would use this frequently; easy to use; well integrated; 
learn quickly; confident using it.

Negative items (2,4,6,8,10): 
unnecessarily complex; need support; inconsistent; 
cumbersome; need to learn a lot.

Note: Negative items reverse-coded in scoring.
\end{verbatim}

\subsubsection{Post-Task Survey}
\begin{verbatim}
After each task: 
1. Difficulty: [ ]Easy [ ]Medium [ ]Hard
2. VoiceCLI help? [ ]Yes [ ]No [ ]Partially
3. Confidence: [ ]Very [ ]Confident [ ]Neutral 
   [ ]Uncertain [ ]Not
4. Frustrations (open):
\end{verbatim}

\subsubsection{Demographic Questionnaire}
\begin{verbatim}
1. Age: ____ years  
2. Gender: [ ]Male [ ]Female [ ]Non-binary [ ]Prefer not to say
3. CLI usage: [ ]Daily [ ]Weekly [ ]Monthly [ ]Rarely
4. Primary OS: [ ]Windows [ ]Linux [ ]macOS
5. Voice assistant: [ ]Yes [ ]No  
6. Accessibility: [ ]Yes(specify:____) [ ]No
\end{verbatim}

\subsubsection{Open-Ended Feedback}

\begin{description}
\item[1.] {Most valuable VoiceCLI feature?}
\item[2.] {Improvements for complex tasks?}
\item[3.] {Safety concerns?}
\item[4.] {Would you use this daily? Why/why not?}
\end{description}

\subsection{Performance Summary Reference}
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{Value} & \textbf{Reference Section} \\
\hline
Overall Success Rate & 73.20\% & Section~\ref{sec:results} \\
First Attempt Success & 58.60\% & Section~\ref{sec:results} \\
After Retry/Edit Success & 14.70\% & Section~\ref{sec:results} \\
Success by Difficulty & 78.47\%, 71.23\%, 67.20\% & Section~\ref{sec:results} \\
Error Distribution & 41.04\%, 35.07\%, 9.70\%, 7.46\%, 6.73\% & Section~\ref{sec:results} \\
ASR Accuracy & 94.23\% & Section~\ref{sec:results} \\
LLM Top-3 Accuracy & 83.00\% & Section~\ref{sec:results} \\
Average Latency & 8.30s (SD: 2.00s) & Section~\ref{sec:results} \\
SUS Score & 75.10 (95\% CI: 72.5–77.7) & Section~\ref{sec:results} \\
\hline
\end{tabular}
\caption{Key performance metrics summary with corrected confidence intervals and standard deviations. For detailed calculations, analysis, and verification, see Section~\ref{sec:results} (Results and Analysis).}
\end{table}

\end{document}
