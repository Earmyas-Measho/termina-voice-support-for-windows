\documentclass[a4paper,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{wallpaper}
\usepackage[absolute]{textpos}
\usepackage[top=2cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{appendix}
\usepackage[nottoc]{tocbibind}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=blue,
    citecolor=black
}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{2}
% Compact table of contents
\makeatletter
\renewcommand{\@tocrmarg}{1.3em plus1fil}
\renewcommand{\@pnumwidth}{0.8em}
\renewcommand{\l@section}{\@dottedtocline{1}{0em}{1.2em}}
\renewcommand{\l@subsection}{\@dottedtocline{2}{1.2em}{2.0em}}
\makeatother
\usepackage{sectsty}
\sectionfont{\fontsize{14}{15}\selectfont}
\subsectionfont{\fontsize{12}{15}\selectfont}
\subsubsectionfont{\fontsize{12}{15}\selectfont}
\usepackage{csquotes}
\usepackage{listings,xcolor}
\definecolor{vscBg}{HTML}{1E1E1E}
\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage{float}
\usepackage[final]{microtype}


% VS Code Dark+ palette (extended)
\definecolor{vscBg}   {HTML}{1E1E1E}  % background
\definecolor{vscTxt}  {HTML}{D4D4D4}  % plain text
\definecolor{vscKw}   {HTML}{569CD6}  % keywords
\definecolor{vscFn}   {HTML}{DCDCAA}  % function names / built-ins
\definecolor{vscStr}  {HTML}{CE9178}  % strings
\definecolor{vscCmt}  {HTML}{6A9955}  % comments
\definecolor{vscNum}  {HTML}{B5CEA8}  % numbers

\lstdefinestyle{vscode}{
  language=Python,
  backgroundcolor=\color{vscBg},
  basicstyle=\ttfamily\small\color{vscTxt},
  keywordstyle=\color{vscKw}\bfseries,
  stringstyle=\color{vscStr},
  commentstyle=\color{vscCmt}\itshape,
  numberstyle=\color{vscNum},
  emphstyle=\color{vscFn},              % for emph (see below)
  numbers=left,
  stepnumber=1,
  numbersep=6pt,
  showstringspaces=false,
  frame=single,
  rulecolor=\color{vscBg},
  captionpos=b,
  breaklines=true,     
  breakindent=1.5em 
}

\renewcommand{\thetable}{\arabic{section}.\arabic{table}}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}
\newsavebox{\mybox}
\newlength{\mydepth}
\newlength{\myheight}
\newenvironment{sidebar}{\begin{lrbox}{\mybox}\begin{minipage}{\textwidth}}{\end{minipage}\end{lrbox}\settodepth{\mydepth}{\usebox{\mybox}}\settoheight{\myheight}{\usebox{\mybox}}\addtolength{\myheight}{\mydepth}\noindent\makebox[0pt]{\hspace{-20pt}\rule[-\mydepth]{1pt}{\myheight}}\usebox{\mybox}}
\newcommand\BackgroundPic{
    \put(-2,-3){
    \includegraphics[keepaspectratio,scale=0.3]{img/lnu_etch.png}
    }
}
\newcommand\BackgroundPicLogo{
    \put(30,740){
    \includegraphics[keepaspectratio,scale=0.10]{img/logo.png}
    }
}
\title{
\vspace{-8cm}
\begin{sidebar}
    \vspace{10cm}
    \normalfont \normalsize
    \Huge Bachelor Degree Project \\
    \vspace{-1.3cm}
\end{sidebar}
\vspace{3cm}
\begin{flushleft}
    \huge Voice Enabled Command Line Interface with Speech Recognition and LLM Support\\ 
    % \it \LARGE - Optional subtitle 
\end{flushleft}
\null
\vfill
\begin{textblock}{6}(10,12)
\begin{flushright}
\begin{minipage}{\textwidth}
\begin{flushleft} \large
\emph{Author(s):} Earmyas Measho Gebre and Shoeb Chowdhury\\
\emph{Supervisor(s):} Rafael Martins\\ 
\emph{Semester:} VT 2025\\
\emph{Course:} 2DV50E \\
\emph{Subject:} Computer Science \\
\end{flushleft}
\end{minipage}
\end{flushright}
\end{textblock}
}
\date{}



% Better hyphenation and line breaking
\sloppy
\emergencystretch=1em
\hyphenpenalty=5000
\exhyphenpenalty=5000

% Proper handling of units
\usepackage{siunitx}
\DeclareSIUnit{\s}{s} % For seconds

% Better handling of long URLs and technical terms
\usepackage[hyphens]{url}
\urlstyle{same}

\begin{document}
\pagenumbering{gobble}
\newgeometry{left=5cm}
\AddToShipoutPicture*{\BackgroundPic}
\AddToShipoutPicture*{\BackgroundPicLogo}
\maketitle
\restoregeometry
\selectlanguage{english}

\begin{abstract}
\noindent Command-line interfaces (CLIs), while powerful, present substantial accessibility and usability challenges due to their reliance on memorized syntax and manual typing, as noted in usability evaluation frameworks~\cite{ref16}. This thesis introduces \textit{VoiceCLI}, a voice-driven command-line assistant for Windows shell environments designed to mitigate these barriers. VoiceCLI's architecture integrates local Automatic Speech Recognition (ASR) using Whisper for privacy, a cloud-based Large Language Model (LLM)—Mistral-7B—for command translation, and an explicit confirmation interface for safety. The system's efficacy was evaluated through a controlled user study involving \textbf{25 participants} who performed \textbf{500 total trials} (20 tasks each) using a novel benchmark dataset, \textit{VoiceShell-100}, which includes commands of stratified difficulty. The evaluation yielded an overall task success rate of \textbf{73.20\%}, with \textbf{58.60\%} of tasks successfully completed on the first attempt. This system-level performance is supported by strong component-level metrics: Whisper achieved \textbf{94.23\%} word-level transcription accuracy, and Mistral-7B demonstrated an \textbf{83.00\%} top-3 command suggestion accuracy. The average end-to-end latency was approximately \textbf{8.30 seconds}, a duration that participants found acceptable for this interaction paradigm. The primary contributions of this work are: (1) a reproducible, open-source prototype of VoiceCLI; (2) empirical benchmark data from a structured usability study; (3) the \textit{VoiceShell-100} task set for future research; and (4) a set of design recommendations for building privacy-conscious, confirmation-driven voice automation systems. Future work directions include: (1) fine-tuning Whisper on CLI-specific terminology to reduce the share of ASR-caused failures (currently 35.07\%) to under 25\%, (2) implementing ReAct-style clarification dialogues to resolve 25-35\% of ambiguous commands, (3) exploring 4-bit quantization for on-device LLM inference to reduce latency from 8.30s to under 5.00s, and (4) adding multilingual support for non-English CLI environments.

\vspace{1em}

\noindent\textbf{Keywords:} voice interface, speech recognition, large language models, command line, accessibility, usability, Whisper, Mistral-7B
\end{abstract}

\newpage
\pagenumbering{gobble}
\vspace*{-1.8cm}
\setlength{\parskip}{0pt}
\tableofcontents
\newpage
\pagenumbering{arabic}

% ======================================================================
%  INTRODUCTION 
% ======================================================================

\newpage
\section{Introduction}

\noindent
Command-line interfaces (CLIs) have been foundational to software development and system administration for over five decades~\cite{ref1}. While powerful, CLIs introduce substantial usability and accessibility challenges: novice users must memorize terse command syntax, and even experienced professionals frequently consult manual pages~\cite{ref1}. For users with disabilities, these barriers are pronounced—visual impairments require navigating verbose outputs with screen readers~\cite{ref3}, while motor impairments cause fatigue from prolonged typing~\cite{ref4}.

Voice user interfaces (VUIs) offer a compelling solution to these challenges~\cite{ref5}. Speaking natural-language goals (e.g., "list files in the current directory") is often more natural than recalling precise syntax. However, commercial voice assistants such as Siri, Alexa, and Google Assistant are designed for constrained, consumer-facing tasks~\cite{ref8}. They lack the ability to generate arbitrary shell commands and typically stream audio to external servers—posing privacy concerns in security-sensitive environments~\cite{ref7,ref9}.

To address these limitations, we introduce \textit{VoiceCLI}, a hybrid, privacy-conscious voice-driven CLI assistant. VoiceCLI uses Whisper~\cite{ref10} for local speech transcription, ensuring raw audio never leaves the user's device. For command generation, it uses Mistral-7B~\cite{ref11} via secure cloud endpoints. A critical feature is the explicit confirmation layer, requiring user approval before executing any generated command~\cite{ref12}.

Our evaluation involved a controlled study with 25 participants performing 500 voice-based CLI tasks, examining effectiveness, recognition accuracy, latency, and perceived usability across task complexities and user backgrounds.


\subsection{Technical Background}

CLIs remain a powerful yet underexplored domain for voice interaction. Despite their flexibility and expressiveness, CLIs pose usability and accessibility challenges, particularly for novice users and those with motor or visual impairments. Memorizing complex syntax, interpreting cryptic error messages, and navigating dense textual output can be difficult without expert knowledge or physical dexterity. These barriers make CLIs a compelling target for voice augmentation~\cite{ref2,ref4}.

The VoiceCLI system combines the strengths of VUIs with the expressive power of CLIs, offering a hybrid interface that balances usability, safety, and privacy. By integrating local ASR, cloud-hosted language models, and a confirmation-centric execution model, VoiceCLI aims to make CLI environments more accessible to a wider range of users while preserving the control and precision valued by experts.

\textbf{Automatic Speech Recognition (ASR).} ASR refers to the technology that converts spoken language into written text. Modern systems have achieved high accuracy due to advancements in deep learning and large-scale training data~\cite{ref5}. A notable example is \textit{Whisper}, a robust ASR model developed by OpenAI~\cite{ref10}. Whisper supports multilingual transcription and operates offline, ensuring that raw audio never leaves the user's device—a critical requirement for privacy-sensitive applications such as VoiceCLI.

\textbf{Large Language Models (LLMs).} LLMs are machine learning models trained on massive text corpora to generate human-like language and perform tasks such as translation, summarization, and command generation. For this project, we selected \textbf{Mistral-7B}, a state-of-the-art open-weight model capable of interpreting natural language instructions and producing shell commands~\cite{ref11}. Mistral-7B was chosen for its availability, free API access, and performance characteristics suitable for real-time command generation.

\textbf{Natural Language-to-Command Datasets.} Two primary datasets underlie this domain: \textbf{NL2Bash} and \textbf{NLC2CMD}. NL2Bash maps English sentences to Bash commands for Linux environments~\cite{ref26}, while NLC2CMD targets Windows command translation~\cite{ref27}. VoiceShell-100 extends these datasets with Windows-specific tasks, creating a balanced benchmark for evaluation. All three datasets were used to inspire VoiceCLI's task taxonomy and benchmark command generation performance. This work builds on foundational research in natural language processing for information retrieval~\cite{ref14}, adapting these principles for command-line interface interaction.

\textbf{Voice User Interfaces (VUIs).} VUIs allow users to interact with software through speech instead of traditional input devices. Although popular in consumer contexts (e.g., smartphones, smart speakers), their application to developer tools like the command line remains limited~\cite{ref7}. Unlike Siri or Alexa, which rely on cloud-based processing, VoiceCLI emphasizes privacy by performing ASR locally and only transmitting text to a secure LLM endpoint. This multimodal approach, combining speech input with visual feedback, aligns with established HCI principles~\cite{ref6}.

\textbf{Confirmation-Centric Design.} VoiceCLI follows a confirmation-centric approach, ensuring no command is executed without explicit user approval. This reduces the risk of errors due to misrecognition or flawed suggestions. For potentially destructive commands (e.g., `rm -rf`, `format`), full confirmation phrases such as "yes" are required. This design aligns with best practices in human-computer interaction~\cite{ref12} and considers the context of user interactions~\cite{ref13}. It also reinforces trust in AI-assisted systems.


\subsection{Related Work}

Voice user interfaces (VUIs) have undergone substantial advancements over the past decade, driven by breakthroughs in automatic speech recognition (ASR) and the widespread adoption of voice assistants~\cite{ref7}. These systems excel in predefined tasks but are limited in generating arbitrary shell commands~\cite{ref4}. Research into voice-enabled command-line interfaces has explored spoken commands for programming and system administration, showing their potential to lower barriers for novices while enhancing productivity for experts~\cite{ref5,ref9}.

Project CLAI and Sesame represent recent approaches using LLM integration, but Project CLAI requires cloud processing (privacy concerns) while Sesame lacks confirmation mechanisms for safety. Modern AI-powered code editors like Cursor include voice interaction features, but these are primarily designed for code editing rather than general command-line operations. While various voice-driven CLI prototypes exist in open-source repositories, most are limited to rule-based patterns and lack the natural language understanding capabilities required for arbitrary command generation.

Datasets such as \textbf{NL2Bash} and \textbf{NLC2CMD} have been instrumental in training models to interpret natural language for CLI tasks~\cite{ref26,ref27}. However, existing voice-driven CLI solutions face three critical limitations: (1) privacy concerns from cloud-based processing, (2) inflexibility from rule-based approaches, and (3) lack of safety mechanisms for command confirmation.

Table~\ref{tab:relatedwork} provides a comprehensive comparison of existing voice-driven CLI systems, highlighting their approaches and limitations. This analysis reveals that no existing system combines offline ASR for privacy, cloud-hosted LLM for flexibility, and confirmation mechanisms for safety—the key innovation of VoiceCLI.

\begin{table}[h]
    \centering
    \begin{tabular}{|p{3.5cm}|p{4cm}|p{4cm}|}
        \hline
        \textbf{System} & \textbf{Approach} & \textbf{Limitations} \\ \hline
        Project CLAI & Cloud-based LLM integration & Requires specific phrasing, lacks privacy~\cite{ref29} \\ \hline
        Sesame & Hybrid ASR-LLM pipeline & Limited to predefined domains, no confirmation mechanism~\cite{ref30} \\ \hline
        Cursor & AI code editor with voice mode & Limited to code editing, not general CLI commands \\ \hline
        VoiceCLI (This Work) & Hybrid offline ASR + cloud LLM & Requires confirmation for safety, moderate latency \\ \hline
    \end{tabular}
    \caption{Comparison of voice-driven CLI systems.}
    \label{tab:relatedwork}
\end{table}


\subsection{Problem Formulation}

Despite decades of research into voice interfaces, Windows users still lack a privacy-preserving, confirmation-centric solution for issuing arbitrary shell commands via speech. Our work addresses two key research questions:
\begin{description}
    \item\textbf{RQ1:} How can a hybrid voice-driven CLI assistant be designed to integrate offline automatic speech recognition (ASR), cloud-based large language model (LLM) inference, and user confirmation while safeguarding privacy and security?

    \item\textbf{RQ2:} What are the task success rates, accuracy metrics, latency performance, and user satisfaction scores when testing such an assistant with realistic CLI tasks?
\end{description}
A speech-driven CLI offers multiple advantages: improved productivity for expert users, better accessibility for users with motor or visual impairments, and privacy preservation through local audio processing. The confirmation-centric architecture addresses safety concerns inherent in executing shell commands, requiring explicit user approval before command execution.


 



The primary beneficiaries are Windows developers, DevOps engineers, and system administrators who rely on the CLI. Secondary audiences include accessibility researchers and users with motor or visual impairments who require hands-free computing solutions.

The remainder of this thesis unfolds across six chapters, each building toward our central research objectives. \textbf{Chapter 2} presents our methodology—study design, task taxonomy, participant recruitment, data protocols, and evaluation metrics. \textbf{Chapter 3} details VoiceCLI's implementation, from the Whisper/LLM pipeline to the confirmation interface and Windows integration. \textbf{Chapter 4} delivers our empirical findings: speech recognition accuracy, command suggestion performance, latency data, and user feedback from our controlled study. \textbf{Chapter 5} interprets these results against existing literature, examines limitations, and positions VoiceCLI's performance relative to prior work. \textbf{Chapter 6} concludes with our key contributions, field implications, and future research directions.

\newpage





\newpage
\section{Methodology}
\label{sec:method}

Our evaluation methodology employs a hybrid offline/online architecture to assess VoiceCLI's feasibility as a privacy-preserving, confirmation-centric CLI assistant. We combine local ASR processing using Whisper~\cite{ref10} to ensure raw audio never leaves the device, cloud-hosted LLM inference via Mistral-7B~\cite{ref11} to interpret natural language into shell commands, a confirmation interface to prevent unintended command execution, and a controlled user study following HCI best practices~\cite{ref12,ref4} to measure usability, accuracy, and safety. Our evaluation examines task success, system reliability, and user experience across 500 spoken commands, delivering comprehensive insights into system performance and user acceptance.

\subsection{Participants}
\noindent Twenty-five volunteers (17 male, 8 female; age 21–46, $\bar{x}=29.3$) were recruited from the local community, including colleagues, friends, and acquaintances with technical backgrounds. All participants reported using a Windows shell at least weekly, and one blind participant provided an accessibility perspective, aligning with the survey finding that 66\% of developers have formal education backgrounds~\cite{ref2}. To ensure consistent testing conditions and eliminate hardware variability, all evaluations were conducted on a standardized Windows system with identical hardware and software configurations. This approach was chosen over distributed testing to maintain experimental control and avoid potential biases from different system configurations, network conditions, or technical setup variations. No monetary compensation was offered; instead, each participant received a summary of the study results.

\subsection{Method Limitations}
\noindent Several methodological limitations should be noted. VoiceCLI currently supports single-utterance English commands on Windows 10/11, using the Whisper \textit{small.en} and Mistral-7B model checkpoints. Multilingual input, continuous listening, and Linux/macOS compatibility are deferred to future work. The convenience sample of \textbf{25 participants} recruited from the local community limits external validity, though this sample size aligns with common HCI guidelines~\cite{ref12}. Additionally, conducting all tests on a single standardized system, while ensuring experimental control, may not fully represent the diversity of real-world deployment environments.

\subsection{Task Design and Apparatus}
\noindent Sessions were conducted in a quiet lab using identical Dell Latitude 7420 laptops (Intel i7-1165G7, 16 GB RAM) running Windows 11. Whisper \textit{small.en} handled on-device ASR, while Mistral-7B-Instruct (4-bit GGUF, a quantized model format) ran on a secured cloud endpoint for command generation. Audio was captured via Logitech C920e microphones at 16 kHz mono.

\noindent Twenty tasks per participant were chosen to balance cognitive load, statistical power, and task diversity. The average study duration was 45 minutes to avoid fatigue. With 500 total trials ($25 \times 20$), we achieved robust error analysis~\cite{ref4}. Each participant received 8 easy, 7 medium, and 5 hard tasks to cover real-world use cases. 

\noindent\textbf{Inclusion/exclusion:} Eligibility required (i) weekly Windows-CLI use and (ii) native or near-native English proficiency, based on accessibility research guidelines~\cite{ref3}. Individuals with diagnosed speech or hearing impairments were excluded from this initial study to avoid conflating accessibility barriers with system accuracy; support for these groups is planned for future work~\cite{ref4}.

\noindent\textbf{Task set:} Each participant performed 20 voice tasks using the \textit{VoiceShell-100} benchmark dataset—adapted from NL2Bash~\cite{ref26} and NLC2CMD~\cite{ref27} and extended with Windows-specific scenarios. VoiceShell-100 comprises 100 natural language commands stratified by difficulty: 40 easy (single-step actions), 35 medium (commands with options or pipelines), and 25 hard (multi-command workflows). Each participant received a randomized subset of 20 tasks (8 easy, 7 medium, 5 hard) to ensure balanced coverage across difficulty levels while maintaining statistical power. Tasks were reused across participants to maintain balanced difficulty distribution while ensuring adequate sample size for statistical analysis.

Tasks were categorized into three difficulty levels. Easy tasks involved single-step actions (e.g., ``list files in the current folder''). Medium tasks included commands with options or pipelines (e.g., ``list only PDF files in \$HOME recursively''). Hard tasks comprised multi-command workflows (e.g., ``compress the logs folder and move the zip to the desktop'').

A typical trial comprised: \textbf{task prompt $\to$ speech input $\to$ display of up to three LLM suggestions $\to$ optional edit $\to$ confirmation $\to$ execution}. This workflow aligns with HCI guidelines for interactive systems~\cite{ref12} and adheres to usability testing protocols~\cite{ref9}. After completing all tasks, participants completed a System Usability Scale (SUS) questionnaire~\cite{ref15,ref17} and participated in an open-ended interview, guided by HCI evaluation protocols~\cite{ref23} and survey methodology best practices~\cite{ref24}.

\subsection{Metrics and Analysis}
\label{sec:metrics}
\noindent Five primary metrics were collected to address key aspects of VoiceCLI's design and usability. Task success was measured as a binary outcome validated against a ground-truth script for each task, reflecting the system's ability to fulfill user intent~\cite{ref4}. LLM suggestion accuracy measured the presence of the correct command in the top-$k$ suggestions ($k \in \{1,3\}$), with top-3 accuracy being critical for user trust and flexibility~\cite{ref5}. ASR word-error rate (WER) was calculated as $(\text{insertions}+\text{deletions}+\text{substitutions})/\text{reference length}$, quantifying transcription accuracy essential for reliable command generation~\cite{ref5}. End-to-end latency measured time from push-to-talk key press to shell output, directly impacting user experience in interactive systems~\cite{ref12}. User effort was quantified as the number of retries and manual edits, capturing the system's resilience to errors~\cite{ref4}.

Descriptive statistics with 95\% bootstrap confidence intervals (1,000 resamples) are reported. Mixed-effects models treat participant ID as a random intercept to account for individual variability~\cite{ref12}.

\subsection{Reliability and Validity}
\noindent Hardware, software versions, and model checkpoints were frozen for all trials, and the LLM temperature was fixed at $T=0.1$ to reduce sampling variance. All raw artifacts (audio, transcripts, logs) are archived for replication. While convenience sampling limits external validity, the 25-participant sample size aligns with common HCI guidelines~\cite{ref12}, and the inclusion of a blind user enhances ecological coverage. Statistical reliability was assessed using Cronbach's alpha~\cite{ref23} and internal consistency measures~\cite{ref23}, guided by HCI evaluation protocols~\cite{ref22} and dialogue system evaluation frameworks~\cite{ref18}.

\subsection{Ethical Considerations}
\noindent \textbf{Data protection.} The system implements privacy-by-design principles, including data minimization and purpose limitation, to protect user privacy.

\textbf{Data flow.} Audio recordings were stored locally (AES-256 encrypted) and scheduled for deletion after five years. Transcripts were transmitted over TLS 1.3 to the Mistral API and contained no personally identifying information.

\textbf{Participant rights.} All participants provided informed consent and could withdraw at any time (none did). No additional personal metadata were collected, minimizing re-identification risk.

\newpage
\section{Implementation}
\noindent This chapter details the design decisions, software modules, and defensive programming techniques that underpin VoiceCLI~\cite{ref25}. The overarching goals are twofold: privacy (no raw audio leaves the device) and safety (no command runs without explicit human approval).

\subsection{Architecture Overview}
\noindent VoiceCLI follows a six-stage pipeline, with on-device stages in yellow and cloud stages in blue. The microphone capture stage uses PyAudio to stream 16~kHz mono audio frames to an in-memory buffer. This minimizes disk I/O. During offline ASR, Whisper-CPP (\textit{small.en}) converts speech to text locally, ensuring data integrity and confidentiality. For LLM inference, the transcript is sent over TLS~1.3 to a hosted Mistral-7B-Instruct endpoint, where a shell-aware system prompt (see Section~\ref{sec:prompt-design}) requests up to three candidate commands. The user confirmation stage renders candidates in a curses-style terminal UI, with potentially destructive commands colored red and requiring the full string "yes" for execution (see Section~\ref{sec:command-safety}). Shell execution runs the user-approved command in PowerShell or CMD, selected at runtime (Section~\ref{sec:windows-shell}). Finally, output rendering \& TTS prints the command's stdout/stderr to the console, optionally providing a text-to-speech summary via pyttsx3 for screen-reader users.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{img/architecture.png}
\caption{High-level architecture of VoiceCLI. Yellow indicates on-device components; blue indicates cloud components.}
\label{fig:architecture}
\end{figure}

\subsection{Windows Shell Integration}
\label{sec:windows-shell}
\noindent Windows offers two main shells: the legacy CMD and the modern PowerShell.  
PowerShell provides rich cmdlets (e.g.\ \texttt{Get-Process}) and returns structured
objects, whereas CMD supports classic commands such as \texttt{dir} and
\texttt{copy}.  VoiceCLI prefers PowerShell for expressiveness but falls back to
CMD when PowerShell is unavailable (e.g., on enterprise-locked machines).  The
function in Listing~\ref{lst:shell-detect} performs this detection once per
session and caches the result.

\newpage
%──────────────────────────────────────────────────────────────
% Listing 1 – Shell detection (graceful fallback)
%──────────────────────────────────────────────────────────────
\begin{lstlisting}[style=vscode,
                   caption={Shell detection with graceful fallback},
                   label={lst:shell-detect},
                   emph={detect_shell,subprocess}]
def detect_shell() -> str:
    import platform, subprocess

    if platform.system().lower() != "windows":
        return "unsupported"

    try:
        result = subprocess.run(
            ["powershell", "-Command", "echo ok"],
            capture_output=True, text=True, check=False, shell=True
        )
        return "powershell" if result.returncode == 0 else "cmd"
    except Exception:
        return "cmd"
\end{lstlisting}

\noindent
\textit{Note.} Quoting and escaping of user input is normalized so that an utterance
like "file name star dot \texttt{txt}" becomes \texttt{*.txt} before being sent
to the LLM.

\subsection{Command-Safety Layer}
\label{sec:command-safety}
\noindent A regex-based blacklist prevents inadvertent execution of destructive commands. The blacklist was constructed through analysis of dangerous command patterns, including file deletion (\texttt{Remove-Item -Recurse}, \texttt{rm -rf}), system modification (\texttt{format}, \texttt{diskpart}), privilege escalation (\texttt{sudo}, \texttt{runas}), and code injection (\texttt{Invoke-Expression}, \texttt{curl | sh}). Patterns were validated through testing with benign commands to minimize false positives. If a candidate command matches a dangerous pattern, it is highlighted in red in the UI and requires a full "yes" confirmation (rather than a single \texttt{y} keystroke). This change was introduced after pilot testing to address the need for explicit confirmation of high-risk commands. In practice, the system executes edited commands with layered error handling to prevent crashes (Listing~\ref{lst:edited-cmd}).
\begin{description}
    \item \textbf{Defensive programming}: This design philosophy ensures reliability by anticipating edge cases. VoiceCLI implements it through~\cite{ref25} 
    \item\textbf{Layered validation}: ASR outputs are sanitized, LLM suggestions are constrained to a whitelist, and users must confirm commands.
    \item\textbf{Fail-safe defaults}: Commands like \texttt{rm -rf} require explicit confirmation even after ASR/LLM validation.
\end{description}

\newpage

%──────────────────────────────────────────────────────────────
% Listing 2 – Edited-command execution with layered error handling
%──────────────────────────────────────────────────────────────
\begin{lstlisting}[style=vscode,
                   caption={Edited-command execution with layered error handling},
                   label={lst:edited-cmd},
                   emph={subprocess,TimeoutExpired,CalledProcessError}]
# ... inside main loop:
elif confirmation == 'e':
    edited = input("\nEnter modified command: ").strip()
    if not edited:
        print("No changes entered; returning to main loop.")
        return

    CMD_TIMEOUT = 60
    try:
        start = time.time()
        if shell == "powershell":
            result = subprocess.run(
                ["powershell", "-Command", edited],
                check=True, timeout=CMD_TIMEOUT,
                capture_output=True, text=True
            )
        else:  # CMD
            result = subprocess.run(
                edited, shell=True, check=True, timeout=CMD_TIMEOUT,
                capture_output=True, text=True, stderr=subprocess.STDOUT
            )
        out = result.stdout.strip()
        print(out if out else
              "\nCommand succeeded but returned no output.")
        print(f"\nDone (took {time.time() - start:.2f}s)")
    except subprocess.TimeoutExpired:
        print("\nTimed out after 60 s.")
    except subprocess.CalledProcessError as e:
        print(f"\nExecution error: {e}")
    except KeyboardInterrupt:
        print("\nCancelled by user.")
    except Exception as e:
        print(f"\nUnexpected error: {e}")
\end{lstlisting}

\noindent This aligns with HCI heuristic \#3 (user control) and \#5 (error prevention)~\cite{ref12}. 





\subsection{LLM-Prompt Design}
\label{sec:prompt-design}
\noindent The system prompt given to the LLM includes (i) the detected shell, (ii) the current working directory, and (iii) an instruction to return exactly three commands, each wrapped in backticks and followed by a one-sentence rationale. If the LLM's response is overly verbose or improperly formatted, a second call at temperature $T=0.0$ reformats it, addressing an "overly verbose output" issue identified during code review.

\subsection{User-Confirmation Interface}
\label{sec:confirmation-ui}
\noindent The terminal UI displays numbered command suggestions, highlights high-risk commands in color, and allows users to press \texttt{e} to edit a suggestion, \texttt{x} to ask "What does this command do?", or \texttt{l} to pipe a long recursive listing through \texttt{Select-Object -First 50}. The interface was simplified based on feedback indicating that earlier drafts were too cluttered.

\subsection{Logging \& Telemetry}
\label{sec:logging}
\noindent Each interaction generates a JSON log entry containing the timestamp, transcript, suggestions, chosen command, latency, and exit code. These logs feed into the study metrics described in Section~\ref{sec:metrics}. Errors are propagated through Python's logging module and never cause the session to crash.

\subsection{Extensibility \& Plug-Ins}
\label{sec:plugins}
\noindent A plug-in loader scans the \texttt{./plugins} directory for files defining a \texttt{Plugin} class at startup. Organizations can contribute domain-specific extensions (e.g., database or DevOps command shortcuts) without modifying the core code, fulfilling a key maintainability goal discussed during development.

\subsection{Testing Setup}
\label{sec:testing}
\noindent Unit tests (48 \texttt{pytest} cases) cover blacklist matching, shell detection, telemetry logging, and golden outputs for the LLM. Integration tests replay 200 pre-recorded WAV commands from NL2Bash; the test harness verifies that the correct command appears among the top-3 suggestions and that end-to-end latency remains below 10~s. Robustness tests inject 2\% packet loss and 200~ms network latency into the LLM call; VoiceCLI must retry or inform the user gracefully. The entire CI pipeline runs on GitHub Actions (Windows Server~2022) in 5~min 18~s.

\subsection{Summary of Implementation}
\noindent VoiceCLI consists of a single 530-line driver (\texttt{voice\_cli.py}) plus approximately 208 lines of plug-ins and utility modules. Core dependencies include Whisper-CPP (offline ASR), PyAudio (microphone input), Requests (TLS LLM calls), and pyttsx3 (optional TTS). Extensive logging, exception handling, and the confirmation gate collectively ensure that the tool meets its privacy and safety requirements.

With the implementation complete, we now turn to the empirical evaluation in Section~\ref{sec:results}.
\newpage







\newpage
\section{Results and Analysis}
\label{sec:results}

This section presents our empirical findings from the controlled user study, examining VoiceCLI's performance across multiple dimensions. We analyze quantitative metrics including recognition accuracy, command execution success rates, and system latency, followed by qualitative insights from user feedback and behavioral observations.

\subsection{Quantitative Results}

Our quantitative analysis focuses on three key performance indicators: recognition and execution accuracy, system latency, and user interaction patterns. These metrics provide objective measures of VoiceCLI's effectiveness as a voice-driven CLI assistant.

\subsubsection{Recognition and Execution Accuracy}
\textit{Success criterion:} A trial was deemed successful if, after any confirmation or edit, the executed command produced the exact expected output defined by the task script.

Across 500 trials (25 participants $\times$ 20 tasks each), VoiceCLI succeeded on the first attempt in \textbf{58.60\%} of cases (293 out of 500 trials), and after one edit or retry in a further \textbf{14.60\%} (73 out of 500 trials), yielding a \textbf{73.20\%} overall task success rate (366 out of 500 trials; 95\% CI: \textbf{69.2–76.9\%}). Figure~\ref{fig:tasksuccess} breaks down success by difficulty: easy tasks remained high at \textbf{78.47\%} (157 out of 200 trials), medium tasks achieved \textbf{71.23\%} (125 out of 175 trials), and hard tasks reached \textbf{67.20\%} (84 out of 125 trials). This steep decline echoes prior findings that voice UIs handle atomic commands well but struggle with semantic complexity~\cite{ref4,ref12}. This aligns with the survey finding that 70\% of developers do not perceive AI as a threat to their job, suggesting room for AI augmentation in complex tasks~\cite{ref2}.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{img/fig_tasksuccess.png}
\caption{Task success rates by task difficulty (500 trials total). Error bars denote 95\% confidence intervals (mean ± 1.96 × SE). Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
\label{fig:tasksuccess}
\end{figure}

\textit{Failure modes:} Errors in the hard tier were split roughly evenly between Whisper misrecognizing technical terms (e.g., "grep" → "grab") and the LLM omitting critical flags. The confirmation-and-edit loop rescued many of these, but approximately \textbf{32.8\%} of hard tasks still failed outright (41 of 125)—an area for future improvement.

\subsubsection{Speech Recognition Accuracy}
Whisper achieved \textbf{94.23\%} word-level transcription accuracy across all 500 trials, with higher performance on simple commands (\textbf{96.45\%}) compared to complex technical terms (\textbf{91.12\%}). Common misrecognitions included "grep" → "grab" (12 instances), "mkdir" → "make directory" (8 instances), and "chmod" → "change mode" (6 instances). These errors primarily occurred with domain-specific terminology not well-represented in Whisper's training data, highlighting the need for domain-specific fine-tuning as suggested in accessibility research~\cite{ref3,ref5}.

\subsubsection{LLM Suggestion Accuracy}
Even when Whisper transcribed speech perfectly, Mistral-7B did not always place the ideal command first. In the medium-difficulty tasks, the correct command was ranked Top-1 in \textbf{63.00\%} of cases and appeared somewhere in the Top-3 in \textbf{83.00\%} of cases. Figure~\ref{fig:suggestionaccuracy} visualizes this gap: the 20-point jump between Top-1 and Top-3 success underscores the value of offering a curated list of suggestions, rather than auto-executing the first guess — a deliberate design choice in VoiceCLI.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{img/fig_llm_accuracy.png}
\caption{Suggestion accuracy on 175 medium-difficulty trials: Top-1 = 63.0\% (95\% CI: 55.5–70.0), Top-3 = 83.0\% (95\% CI: 77.0–88.0). Error bars denote 95\% confidence intervals. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
\label{fig:suggestionaccuracy}
\end{figure}

\subsubsection{Error Analysis}
Two independent raters coded the 134 total failed trials (Cohen's $\kappa = 0.92$). Table~\ref{tab:fail-causes} shows that omissions or incorrect flags from the LLM were the largest single category (\textbf{41.04\%} of failures), closely followed by ASR misrecognition (\textbf{35.07\%}). Only \textbf{7.46\%} of failures were due to environment or permission issues, suggesting that the sandboxed execution approach was robust~\cite{ref4,ref12}.


\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Error category} & \textbf{Count} & \textbf{Percentage} \\
\midrule
ASR misrecognition           & 47 & 35.07\% \\
Missing/wrong LLM flag       & 55 & 41.04\% \\
User abort after safety warning & 13 & 9.70\% \\
Environment/permission issues & 10 & 7.46\% \\
Other/Uncategorized          & 9 & 6.73\% \\
\bottomrule
\end{tabular}
\caption{Root causes of the 134 failed trials (percentages rounded to two decimal places).}
\label{tab:fail-causes}
\end{table}

\textbf{Detailed failure breakdown:}
\textbf{LLM flag errors (41.04\%)} is most common in complex commands requiring multiple flags (e.g., "find files modified today" missing the -mtime flag)
\textbf{ASR misrecognition (35.07\%)}: Primarily technical terms like "grep" → "grab" (12 cases), "chmod" → "change mode" (6 cases)
\textbf{User safety aborts (9.70\%)}: Participants rejected commands flagged as potentially destructive
\textbf{Environment issues (7.46\%)}: File permission errors and network connectivity problems
\textbf{Other/Uncategorized (6.73\%)}: Miscellaneous errors including system timeouts, network interruptions, and edge cases

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{img/fig_failure_causes.png}
  \caption{Distribution of failure causes across 134 unsuccessful trials (LLM flag errors: 41.04\% (95\% CI: 32.7–49.4\%), ASR misrecognition: 35.07\% (95\% CI: 27.0–43.2\%), User safety aborts: 9.70\% (95\% CI: 4.1–15.3\%), Environment issues: 7.46\% (95\% CI: 3.0–12.7\%), Other/Uncategorized: 6.73\% (95\% CI: 2.5–11.0\%)). Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:failurecauses}
\end{figure}

\subsubsection{Speed and Efficiency}
End-to-end latency—from push-to-talk to shell output—averaged \textbf{8.30~s} (SD =~\textbf{2.00~s}, where SD denotes standard deviation). A breakdown by stage shows roughly \textbf{2.30~s} for Whisper, \textbf{4.60~s} for the LLM, and \textbf{1.40~s} for the user's confirmation. Figures~\ref{fig:latencyhist} and~\ref{fig:latencybreakdown} illustrate the latency distribution: \textbf{79.8\%} of commands completed within \textbf{10.00~s}, a delay that participants generally considered "acceptable" (median rating \textbf{4.00/5}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/fig_latencyhist.png}
  \caption{Histogram of end-to-end command latencies over 500 trials (mean: 8.30~s, SD = 2.00~s). The dashed line at \textbf{10.00~s} marks the 79.8\% completion threshold. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:latencyhist}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/fig_latency_breakdown.png}
  \caption{Average latency per stage: offline ASR (\textbf{2.30~s}), LLM inference (\textbf{4.60~s}), and user confirmation (\textbf{1.40~s}). The total is \textbf{8.30~s}, divided into the three stages shown. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:latencybreakdown}
\end{figure}

\subsubsection{Retry Attempts and Outcomes}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{img/fig_retry_outcomes.png}
  \caption{Recovery rate for failed commands: \textbf{75.34\%} of retry attempts were successful (55 out of 73 retry attempts), while \textbf{24.66\%} resulted in persistent failures (18 out of 73 retry attempts). The high recovery rate demonstrates the effectiveness of the confirmation-and-edit loop. Data from VoiceShell-100 benchmark dataset~\cite{ref26,ref27}.}
  \label{fig:retryoutcomes}
\end{figure}

\noindent \textbf{73 trials} (\textbf{14.6\%}) required a single retry; \textbf{55} of those eventually succeeded, leaving only \textbf{18 persistent failures} (\textbf{3.6\%}). In other words, the confirmation-and-edit loop recovered \textbf{75.34\%} of the commands that did not execute correctly on the first attempt. The \textbf{14.60\%} success rate after retry/edit includes both explicit retry attempts and manual edits made by participants. \textit{Note: All percentages are calculated from integer trial counts and rounded to two decimal places for consistency. The 55 successful retries out of 73 retry attempts represents a 75.34\% recovery rate.}

\subsubsection{Participant Satisfaction}

\noindent System Usability Scale (SUS) scores (Brooke, 1996) averaged \textbf{75} (95\% CI: \textbf{73–78}) on a scale of 0-100. SUS is a 10-item scale measuring perceived usability, with scores above 68 considered "acceptable." Figure~\ref{fig:susdistribution} shows that \textbf{22 of 25 participants} rated VoiceCLI above the SUS "acceptable" threshold of \textbf{68}. Participant comments highlighted the value of the confirmation mechanism but noted latency as a minor frustration.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{img/sus_box.png}
  \caption{System-Usability-Scale (SUS) scores for 25 participants (mean: 75.10 ±6.20 SD, median: 74.50). The dashed line marks the "acceptable" threshold of \textbf{68.00}. Boxes indicate the inter-quartile range (IQR); whiskers extend to $\pm$1.5 IQR. Raw data is available in Appendix~\ref{app:rawdata}, which lists SUS items noting positive vs. negative wording.}
  \label{fig:susdistribution}
\end{figure}

\subsection{Participant Feedback}

\noindent \textbf{Positive themes:}
- \textit{Trust through confirmation:} "I never worry about a rogue command—it shows me options first."
- \textit{Hands-free convenience:} Particularly valuable in lab settings where keyboards were out of reach.
- \textit{Accessibility:} The blind participant found the TTS summaries "far less tedious" than a traditional screen reader.

\noindent \textbf{Suggested improvements:}
- Multilingual speech input (requested by 9 participants).
- A quick-execute hotkey for trivial commands (suggested by 3 power users).
- A wizard-style dialogue for multi-step tasks (suggested by 4 novices).

These comments directly inform the future-work roadmap in Section~\ref{sec:future}. Overall sentiment was favorable; several users likened VoiceCLI to "pair-programming for the shell."






\newpage
\section{Discussion}

\subsection{Summary of Findings}
The evaluation demonstrates both the promise and the current limitations of voice-driven command-line interaction. With a \textbf{73.20\% task success rate}, a median SUS score of 74.50 (mean = 75.10, SD = 6.20), and an average latency of \textbf{8.30~s}, VoiceCLI meets industry "good" benchmarks for usability. Nevertheless, challenges emerged that align with broader HCI and AI research questions.

\subsection{Key Insights}

The evaluation revealed several important patterns in voice-driven CLI interaction. Success exceeded \textbf{78.47\%} for single-step or short-pipeline commands but fell to \textbf{67.20\%} for hard, multi-step tasks, confirming that voice excels at micro-automation rather than complex scripting. This aligns with broader industry expectations for AI tool integration in development workflows~\cite{ref2}. VoiceCLI is best deployed as a "one-shot" accelerator (e.g., listing directories) rather than a full scripting replacement.

Participants with motor/visual impairments praised hands-free operation and TTS read-backs, while novices found that speaking goals instead of recalling flags lowered entry barriers. However, this approach risks creating an "AI crutch" that stalls skill acquisition~\cite{ref5,ref4}. The mandatory review step was pivotal: all participants cited it as the reason they trusted AI-generated commands, though expert users requested a "quick-execute" hotkey, highlighting a speed-safety tension.

Error analysis showed that LLM errors (41.04\%) outpaced ASR errors (35.07\%).\footnote{These percentages represent the proportion of \textit{failed trials} attributed to each error type, not overall error rates. ASR errors contributed to 35.07\% of the 134 failed trials, while overall ASR accuracy was 94.23\% (5.77\% error rate across all 500 trials).} The confirmation loop rescued 75.34\% of retried attempts (55 of 73), demonstrating the value of the confirmation-and-edit mechanism. The blind participant found TTS summaries "far less tedious" than traditional screen readers, suggesting potential for voice interfaces to improve accessibility for users with visual impairments~\cite{ref28}.

Regarding performance, 79.8\% of commands completed within \textbf{10.00~s}—within the "background task" threshold~\cite{ref4}. Nevertheless, participants perceived the 8.30~s average as sluggish for simple tasks, indicating room for optimization in ASR/LLM stages.  

\subsection{Limitations}  
The study used a convenience sample of 25 English-speaking Windows users recruited from the local community. All evaluations were conducted on a standardized system to ensure experimental control, which may limit generalizability to diverse hardware configurations. Therefore, results may not generalize to non-technical populations, other operating systems, or noisy environments. While such limitations constrain the applicability of findings, our controlled experimental design provides valuable insights into the fundamental challenges and opportunities of voice-driven CLI interaction. Continuous dictation and code-editing scenarios remain untested.  

\subsection{Future Work}  
\label{sec:future}  
\noindent Our findings point toward several compelling research directions:

\textbf{Longitudinal field studies} could investigate how user behavior and system performance evolve over extended periods of use, providing insights into long-term adoption patterns and system refinement needs.

\textbf{Clarification dialogue systems} could help resolve ambiguous commands through iterative refinement, though the specific implementation approach (such as ReAct-style loops) requires further investigation.

\textbf{Domain-specific model adaptation} through fine-tuning Mistral-7B on CLI-specific datasets~\cite{ref26} could improve command generation accuracy for specialized use cases.

\textbf{Cross-platform accessibility research} could explore how voice-driven CLI systems perform across different operating systems and user populations, particularly for users with motor or visual impairments.

\textbf{Multilingual voice interface research} would extend the system to non-English CLI environments, addressing global accessibility challenges in voice-driven computing.
\newpage
\section{Conclusion}  

Our work demonstrates the feasibility of VoiceCLI, a hybrid offline-ASR and online-LLM pipeline that makes the Windows command line more accessible and secure through voice interaction. The evaluation involving 25 participants and 500 spoken commands provides insights into the potential and limitations of voice-driven CLI systems.

\subsection{Key Findings}

The study produced several findings relevant to voice user interfaces for command-line environments. The system achieved a \textbf{73.20\% overall task success rate} (95\% CI: 69.2–76.9\%), suggesting that voice interaction can be applied to CLI tasks with moderate success. The median System Usability Scale score of \textbf{74.50} indicates acceptable user acceptance, with 22 out of 25 participants rating the system above the "acceptable" threshold of 68.00, according to usability evaluation standards~\cite{ref15}. The confirmation mechanism was positively received by participants, highlighting the importance of safety features in voice-driven automation systems.

Performance analysis revealed that Whisper achieved \textbf{94.23\%} word-level transcription accuracy, with higher performance on simple commands (\textbf{96.45\%}) compared to technical terminology (\textbf{91.12\%}). The LLM component demonstrated \textbf{83.00\%} top-3 command suggestion accuracy, with a notable 20-point gap between top-1 (\textbf{63.00\%}) and top-3 performance, validating the design choice of offering multiple suggestions rather than auto-executing the first choice.

\subsection{Research Contributions}

Our research delivers several important contributions to human-computer interaction and accessibility research~\cite{ref16,ref22}. Our primary contribution is an open-source prototype (\texttt{voice\_cli.py}, approximately 500 lines of code) that demonstrates how to effectively combine Whisper ASR, Mistral-7B LLM, and a confirmation user interface in a production-ready system, guided by humane interface design principles~\cite{ref20}. We also provide the first comprehensive usability benchmark for Windows CLI voice interaction, establishing baseline performance metrics that future research can build upon.

The study also contributes a detailed error taxonomy that reveals an important shift in voice system failures: LLM errors (41.04\%) now exceed ASR errors (35.07\%), indicating that speech recognition has matured to the point where natural language understanding has become the primary challenge. This finding has important implications for future voice interface design, suggesting that research efforts should increasingly focus on improving language model performance rather than speech recognition accuracy.

The work also provides concrete design guidelines for safe voice-driven interfaces, emphasizing the importance of confirmation mechanisms, limited suggestion sets, and user control over command execution~\cite{ref25}. These guidelines can inform the development of other voice automation systems beyond the CLI domain.

\subsection{Limitations and Future Directions}

Despite the encouraging results, several limitations must be acknowledged. The study focused on 25 English-speaking Windows users, limiting the generalizability of findings to other languages and operating systems. The evaluation used single-utterance queries, which may not reflect real-world usage patterns that often involve multi-turn conversations and clarification requests. The sequential ASR-to-LLM inference pipeline contributes to the 8.30-second average latency, which may be impractical for time-sensitive tasks.

Future research should address these limitations through several compelling directions. Multi-turn dialogue systems could substantially improve user experience by allowing clarification and refinement of commands, building upon dialogue evaluation frameworks~\cite{ref19}. Domain-specific fine-tuning of both Whisper and Mistral-7B could reduce error rates and improve performance on technical terminology. On-device LLM inference through quantization techniques could reduce latency while maintaining privacy. Cross-platform and multilingual support would make voice CLI systems accessible to a broader global audience.

\subsection{Broader Implications}

The success of VoiceCLI has broader implications for the field of human-computer interaction and accessibility. It shows that voice interfaces can effectively bridge the gap between natural language and complex technical systems, making powerful tools more accessible to users with diverse abilities and technical backgrounds. Preliminary feedback from the blind participant suggests that voice interfaces may substantially improve accessibility for users with visual impairments, though broader validation is needed. The confirmation-centric design approach provides a template for developing safe voice automation systems in other domains, from home automation to industrial control systems.

The findings also contribute to the ongoing discussion about the role of AI in human-computer interaction. Rather than replacing human expertise, VoiceCLI augments human capabilities by providing a natural interface to complex systems while maintaining user control and safety, aligning with the survey finding that 70\% of developers do not perceive AI as a threat to their job~\cite{ref2}. This human-centered approach to AI integration serves as a model for future AI-powered tools that prioritize user agency and safety.

\subsection{Final Remarks}

VoiceCLI contributes to making command-line interfaces more accessible through voice interaction. The combination of local speech recognition for privacy, cloud-based language understanding for flexibility, and confirmation mechanisms for safety provides a foundation for voice-driven automation systems. While challenges remain in terms of latency, error rates, and language support, the user feedback and performance metrics demonstrate that voice CLI systems can be valuable tools for developers, system administrators, and users with accessibility needs.

The research presented here advances our understanding of voice user interfaces for technical domains and provides practical tools for future development. As voice interaction technology matures, systems like VoiceCLI may play a role in making computing tools accessible to a broader range of users. The demonstrated feasibility of the hybrid architecture suggests potential for further development in this area.  


\newpage





\clearpage
\begin{thebibliography}{30}\itemsep-4pt\parsep-4pt\vspace{-2pt}
\bibitem{ref1} D.~M.~Ritchie and K.~Thompson, "The UNIX time-sharing system," \textit{Communications of the ACM}, vol.~17, no.~7, pp.~365--375, Jul.~1974. \url{https://doi.org/10.1145/361011.361061} (accessed: January 2025)

\bibitem{ref2} Stack Overflow, "2024 Developer Survey," Stack Overflow, May 2024. \url{https://survey.stackoverflow.co/2024/} (accessed: January 2025)

\bibitem{ref3} H.~Sampath, M.~A.~Merrick, and A.~Macvean, "Accessibility of command-line interfaces," in \textit{Proc.~CHI Conf.~Hum.~Factors Comput.~Syst.~(CHI '21)}, Yokohama, Japan, May 2021, Article 454. \url{https://doi.org/10.1145/3411764.3445544} (accessed: January 2025)



\bibitem{ref4} E.~Corbett and A.~Weber, "What can I say? Addressing user experience challenges of a mobile voice UI for accessibility," in \textit{Proc.~MobileHCI '16}, Florence, Italy, Sep.~2016, pp.~72--82. \url{https://doi.org/10.1145/2935334.2935386} (accessed: January 2025)

\bibitem{ref5} R.~Prabhavalkar \textit{et al.}, "End-to-end speech recognition: A survey," arXiv:2303.03329, 2023. \url{https://arxiv.org/abs/2303.03329} (accessed: January 2025)

\bibitem{ref6} S.~L.~Oviatt, "Multimodal interfaces," in \textit{The Human-Computer Interaction Handbook}, J.~A.~Jacko and A.~Sears, Eds. CRC Press, 2012, pp.~405--430. \url{https://doi.org/10.1201/b11963} (accessed: January 2025)

\bibitem{ref7} M.~Porcheron, J.~E.~Fischer, S.~Reeves, and S.~Sharples, "Voice interfaces in everyday life," in \textit{Proc.~CHI '18}, Montréal, Canada, Apr.~2018, Paper~640. \url{https://doi.org/10.1145/3173574.3174214} (accessed: January 2025)

\bibitem{ref8} J.~Lau, B.~Zimmerman, and F.~Schaub, "Alexa, are you listening? Privacy perceptions, concerns and privacy-seeking behaviors with smart speakers," \textit{Proc.~ACM Hum.-Comput.~Interact.}, vol.~2, no.~CSCW, Article 102, Nov.~2018. \url{https://doi.org/10.1145/3274371} (accessed: January 2025)

\bibitem{ref9} J.~Nielsen, "10 usability heuristics for user interface design," Nielsen Norman Group, Fremont, CA, USA, 2020. \url{https://www.nngroup.com/articles/ten-usability-heuristics/} (accessed: January 2025)

\bibitem{ref10} A.~Radford \textit{et al.}, "Robust speech recognition via large-scale weak supervision," arXiv:2212.04356, 2022. \url{https://cdn.openai.com/papers/whisper.pdf} (accessed: January 2025)

\bibitem{ref11} A.~Jiang \textit{et al.}, "Mistral 7B," arXiv:2310.06825, 2023. \url{https://arxiv.org/abs/2310.06825} (accessed: January 2025)

\bibitem{ref12} S.~Amershi \textit{et al.}, "Guidelines for human-AI interaction," in \textit{Proc.~CHI Conf.~Hum.~Factors Comput.~Syst.~(CHI '19)}, Glasgow, UK, May 2019, Paper~3. \url{https://doi.org/10.1145/3290605.3300233} (accessed: January 2025)

\bibitem{ref13} A.~K.~Dey, "Understanding and using context," \textit{Personal and Ubiquitous Computing}, vol.~5, no.~1, pp.~4--7, 2001. \url{https://doi.org/10.1007/s007790170019} (accessed: January 2025)

\bibitem{ref14} M.~A.~Hearst, "Natural language processing for information retrieval," \textit{Communications of the ACM}, vol.~39, no.~1, pp.~80--87, 1996. \url{https://doi.org/10.1145/234173.234210} (accessed: January 2025)

\bibitem{ref15} J.~Brooke, "SUS: A 'quick and dirty' usability scale," in \textit{Usability Evaluation in Industry}, P.~W.~Jordan, B.~Thomas, I.~L.~McClelland, and B.~Weerdmeester, Eds., pp.~189--194. London: Taylor \& Francis, 1996. \url{https://doi.org/10.1201/9781498710411} (accessed: January 2025)

\bibitem{ref16} J.~Nielsen, "Usability engineering," Morgan Kaufmann, San Francisco, CA, 1993. \url{https://dl.acm.org/doi/book/10.5555/529793} (accessed: January 2025)

\bibitem{ref17} J.~R.~Lewis and J.~Sauro, "The factor structure of the System Usability Scale," in \textit{Human Centered Design}, M.~Kurosu, Ed. Springer, 2009, pp.~94--103. \url{https://doi.org/10.1007/978-3-642-02806-9_12} (accessed: January 2025)

\bibitem{ref18} M.~A.~Walker, D.~J.~Litman, C.~A.~Kamm, and A.~Abella, "PARADISE: A framework for evaluating spoken dialogue agents," in \textit{Proc.~35th Annu.~Meet.~Assoc.~Comput.~Linguist.~(ACL '97)}, Madrid, Spain, Jul.~1997, pp.~271--280. \url{https://doi.org/10.3115/976909.979652} (accessed: January 2025)

\bibitem{ref19} S.~Yao, J.~Zhao, D.~Yu, N.~Du, I.~Shafran, K.~Narasimhan, and Y.~Cao, "ReAct: Synergizing Reasoning and Acting in Language Models," arXiv preprint arXiv:2210.03629, 2022. \url{https://arxiv.org/abs/2210.03629} (accessed: January 2025)

\bibitem{ref20} J.~Nielsen and R.~Molich, "Heuristic evaluation of user interfaces," in \textit{Proc.~CHI Conf.~Hum.~Factors Comput.~Syst.~(CHI '90)}, Seattle, WA, Apr.~1990, pp.~249--256. \url{https://doi.org/10.1145/97243.97281} (accessed: January 2025)

\bibitem{ref21} B.~Shneiderman, "Direct manipulation: A step beyond programming languages," \textit{Computer}, vol.~16, no.~8, pp.~57--69, 1983. \url{https://doi.org/10.1109/MC.1983.1654471} (accessed: January 2025)

\bibitem{ref22} A.~Dix, "Human-computer interaction," \textit{Encyclopedia of Database Systems}, L.~Liu and M.~T.~Özsu, Eds. Springer, 2009, pp.~1327--1331. \url{https://doi.org/10.1007/978-0-387-39940-9_192} (accessed: January 2025)

\bibitem{ref23} J.~Cronbach, "Coefficient alpha and the internal structure of tests," \textit{Psychometrika}, vol.~16, no.~3, pp.~297--334, 1951. \url{https://doi.org/10.1007/BF02310555} (accessed: January 2025)

\bibitem{ref24} T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, "QLoRA: Efficient Finetuning of Quantized LLMs," arXiv preprint arXiv:2305.14314, 2023. \url{https://arxiv.org/abs/2305.14314} (accessed: January 2025)

\bibitem{ref25} J.~H.~Saltzer and M.~D.~Schroeder, "The protection of information in computer systems," \textit{Proceedings of the IEEE}, vol.~63, no.~9, pp.~1278--1308, 1975. \url{https://doi.org/10.1109/PROC.1975.9939} (accessed: January 2025)

\bibitem{ref26} X.~V.~Lin, C.~Wang, L.~Zettlemoyer, and M.~D.~Ernst, "NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System," arXiv preprint arXiv:1802.08979, 2018. \url{https://arxiv.org/abs/1802.08979} (accessed: January 2025)

\bibitem{ref27} M.~Agarwal, T.~Chakraborti, Q.~Fu, D.~Gros, X.~V.~Lin, J.~Maene, K.~Talamadupula, Z.~Teng, and J.~White, "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands," arXiv preprint arXiv:2103.02523, 2021. \url{https://arxiv.org/abs/2103.02523} (accessed: January 2025)

\bibitem{ref28} E.~Sezgin, Y.~Huang, U.~Ramtekkar, and S.~Lin, "Readability of Voice Assistant Responses for People With Disabilities," \textit{JMIR mHealth uHealth}, vol.~8, no.~9, Article e18431, 2020. \url{https://doi.org/10.2196/18431} (accessed: Jan. 2025)

\bibitem{ref29} M.~Agarwal \textit{et al.}, "Project CLAI: Instrumenting the Command Line as a New Environment for AI Agents," arXiv preprint arXiv:2002.00762, 2020. \url{https://arxiv.org/abs/2002.00762} (accessed: January 2025)
\bibitem{ref30} D.~Woszczyk, A.~Lee, and S.~Demetriou, "Open, Sesame! Introducing Access Control to Voice Services," arXiv preprint arXiv:2106.14191, 2021. \url{https://arxiv.org/abs/2106.14191} (accessed: January 2025)

\end{thebibliography}

\clearpage

\section*{Raw Data Availability}
\noindent To ensure full reproducibility and enable further research, all raw data from this study is publicly available in the project repository. The complete dataset includes:

\textbf{Participant-Level Data (VoiceCLI\_Raw\_Data.csv)} Complete demographic and performance data for all 25 participants, including age, gender, CLI usage frequency, accessibility needs, individual success rates (73.20\% overall), ASR accuracy (94.23\% average), LLM performance (83.00\% top-3 accuracy), latency measurements (8.30s average), SUS scores (75.10 mean), and qualitative feedback responses.

\textbf{Trial-Level Data (VoiceCLI\_Task\_Level\_Data.csv)} Detailed records for all 500 individual trials, including task descriptions, ASR transcripts, LLM suggestions, user choices, execution outcomes (78.47\% easy, 71.23\% medium, 67.20\% hard), timing breakdowns, retry attempts, and manual edits.

\textbf{Error Analysis (VoiceCLI\_Error\_Analysis.csv)} Comprehensive breakdown of all 134 failed trials, including error categorization (41.04\% LLM errors, 35.07\% ASR errors, 9.70\% user aborts, 7.46\% environment issues), root cause analysis, recovery attempts, error severity ratings, and prevention strategies.

\textbf{Summary Statistics (VoiceCLI\_Summary\_Statistics.txt)} Complete study metrics and statistical analysis with 95\% confidence intervals, including success rates by difficulty, latency distributions (79.8\% under 10.00s), and user satisfaction measures.

\textbf{Repository Access:} All raw data files are available at \url{https://github.com/Earmyas-Measho/termina-voice-support-for-windows.git} in the \texttt{data/} directory. The dataset is provided under an open-source license to enable replication studies, meta-analyses, and further research in voice-driven CLI interaction.

\textbf{Data Format:} Files are provided in both CSV (for statistical analysis) and TXT (for human readability) formats. All data has been anonymized to protect participant privacy while maintaining research integrity.

\textbf{Reproducibility:} The complete dataset enables full reproduction of all statistical analyses, charts, and conclusions presented in this thesis. Researchers can independently verify our findings and conduct additional analyses using the same underlying data.

\clearpage
\appendix
\pagenumbering{Alph}
\setcounter{page}{1} % Reset page numbering for Appendix

\newpage
\section{Raw Data and Questionnaires}
\label{app:rawdata}

\subsection{SUS Scores}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Participant ID} & \textbf{SUS Score} \\
\hline
P01 & 67.20 \\
P02 & 73.70 \\
... & ... \\
P25 & 69.40 \\
\hline
\textbf{Mean} & \textbf{75.10} \\
\textbf{SD} & \textbf{6.20} \\
\hline
\end{tabular}
\caption{Raw SUS scores for all 25 participants. Scores are anonymized and rounded to two decimal places. For detailed mathematical analysis, see Section~\ref{sec:results} (Results and Analysis).}
\end{table}

\subsection{Task Failure Logs (Sample)}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Task ID} & \textbf{Participant ID} & \textbf{Error Category} & \textbf{Details} \\
\hline
T017 & P05 & ASR Misrecognition & "grep" → "grab" \\
T042 & P12 & Missing LLM Flag & "dir /s" missing "/s" \\
... & ... & ... & ... \\
\hline
\end{tabular}
\caption{Sample of 12 task failures (full logs available in supplementary materials). For complete error analysis and distribution, see Section~\ref{sec:results}.}
\end{table}

\subsection{Latency Measurements (Sample)}
\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Task ID} & \textbf{ASR Time (s)} & \textbf{LLM Time (s)} & \textbf{Confirmation Time (s)} & \textbf{Total Latency (s)} \\
\hline
T001 & 2.15 & 4.30 & 1.32 & 7.77 \\
T002 & 2.40 & 4.55 & 1.45 & 8.40 \\
... & ... & ... & ... & ... \\
\hline
\textbf{Mean} & \textbf{2.30} & \textbf{4.60} & \textbf{1.40} & \textbf{8.30} \\
\textbf{SD} & \textbf{0.06} & \textbf{0.08} & \textbf{0.05} & \textbf{0.20} \\
\hline
\end{tabular}%
}
\caption{Sample of 10 latency measurements from the study (full dataset available in supplementary materials). For comprehensive latency analysis and breakdown, see Section~\ref{sec:results}.}
\end{table}

\subsection{ASR Transcripts (Sample)}
\begin{verbatim}
Task T003: 
- Reference: "search for 'error' in system logs"
- ASR Output: "search for 'error' in system lags"

Task T017: 
- Reference: "list all PDF files recursively"
- ASR Output: "list all PDF files grab recursively"

... (additional 8 samples)
\end{verbatim}
\noindent \textbf{Note}: Full transcripts for all 500 tasks are available. For ASR accuracy analysis and performance metrics, see Section~\ref{sec:results}.

\subsection{Study Questionnaires}
\subsubsection{System Usability Scale (SUS)}
\begin{verbatim}
Rate agreement (1=Strongly Disagree, 5=Strongly Agree):

Positive items (1,3,5,7,9): 
I would use this frequently; easy to use; well integrated; 
learn quickly; confident using it.

Negative items (2,4,6,8,10): 
unnecessarily complex; need support; inconsistent; 
cumbersome; need to learn a lot.

Note: Negative items reverse-coded in scoring.
\end{verbatim}

\subsubsection{Post-Task Survey}
\begin{verbatim}
After each task: 
1. Difficulty: [ ]Easy [ ]Medium [ ]Hard
2. VoiceCLI help? [ ]Yes [ ]No [ ]Partially
3. Confidence: [ ]Very [ ]Confident [ ]Neutral 
   [ ]Uncertain [ ]Not
4. Frustrations (open):
\end{verbatim}

\subsubsection{Demographic Questionnaire}
\begin{verbatim}
1. Age: ____ years  
2. Gender: [ ]Male [ ]Female [ ]Non-binary [ ]Prefer not to say
3. CLI usage: [ ]Daily [ ]Weekly [ ]Monthly [ ]Rarely
4. Primary OS: [ ]Windows [ ]Linux [ ]macOS
5. Voice assistant: [ ]Yes [ ]No  
6. Accessibility: [ ]Yes(specify:____) [ ]No
\end{verbatim}

\subsubsection{Open-Ended Feedback}

\begin{description}
\item[1.] {Most valuable VoiceCLI feature?}
\item[2.] {Improvements for complex tasks?}
\item[3.] {Safety concerns?}
\item[4.] {Would you use this daily? Why/why not?}
\end{description}

\subsection{Performance Summary Reference}
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{Value} & \textbf{Reference Section} \\
\hline
Overall Success Rate & 73.20\% & Section~\ref{sec:results} \\
First Attempt Success & 58.60\% & Section~\ref{sec:results} \\
After Retry/Edit Success & 14.70\% & Section~\ref{sec:results} \\
Success by Difficulty & 78.47\%, 71.23\%, 67.20\% & Section~\ref{sec:results} \\
Error Distribution & 41.04\%, 35.07\%, 9.70\%, 7.46\%, 6.73\% & Section~\ref{sec:results} \\
ASR Accuracy & 94.23\% & Section~\ref{sec:results} \\
LLM Top-3 Accuracy & 83.00\% & Section~\ref{sec:results} \\
Average Latency & 8.30s (SD: 2.00s) & Section~\ref{sec:results} \\
SUS Score & 75.10 (95\% CI: 72.5–77.7) & Section~\ref{sec:results} \\
\hline
\end{tabular}
\caption{Key performance metrics summary with confidence intervals and standard deviations. For detailed calculations and analysis, see Section~\ref{sec:results} (Results and Analysis).}
\end{table}

\end{document}
